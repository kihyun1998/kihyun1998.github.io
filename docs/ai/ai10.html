<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-ai/ai10">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">🎧 Whitepaper Companion Podcast 요약: Foundational LLMs &amp; Text Generation | A Little &quot;Bit&quot;</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://kihyun1998.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://kihyun1998.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://kihyun1998.github.io/docs/ai/ai10"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="🎧 Whitepaper Companion Podcast 요약: Foundational LLMs &amp; Text Generation | A Little &quot;Bit&quot;"><meta data-rh="true" name="description" content="---"><meta data-rh="true" property="og:description" content="---"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://kihyun1998.github.io/docs/ai/ai10"><link data-rh="true" rel="alternate" href="https://kihyun1998.github.io/docs/ai/ai10" hreflang="en"><link data-rh="true" rel="alternate" href="https://kihyun1998.github.io/docs/ai/ai10" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="A Little &quot;Bit&quot; RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="A Little &quot;Bit&quot; Atom Feed"><link rel="stylesheet" href="/assets/css/styles.3b1b8064.css">
<link rel="preload" href="/assets/js/runtime~main.cca82d27.js" as="script">
<link rel="preload" href="/assets/js/main.fe143d0c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/android-icon-48x48.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/android-icon-48x48.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">A Little &quot;Bit&quot;</b></a><a class="navbar__item navbar__link" href="/docs/blog-intro">블로그 배포하기</a><a class="navbar__item navbar__link" href="/docs/design-pattern-intro">개인 공부</a><a class="navbar__item navbar__link" href="/docs/react-intro">FrontEnd</a><a class="navbar__item navbar__link" href="/docs/dart-intro">Dart &amp; Flutter</a><a class="navbar__item navbar__link" href="/docs/go/go1">Go</a><a class="navbar__item navbar__link" href="/docs/backend-basic/backend1">BackEnd</a><a class="navbar__item navbar__link" href="/docs/algorithm/al1">Algorithm</a><a class="navbar__item navbar__link" href="/docs/error/error1">Error</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/ai/ai1">AI</a><a class="navbar__item navbar__link" href="/docs/oracle-cloud/oracle-cloud1">Infra</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/ai/ai1">AI</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai1">RAG (Retrieval Augmented Generation)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai2">Ollama+LLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai3">colab GPU 로컬에서 사용하기</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai4">solar LLM 사용하기</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai5">Open Ko llm leaderboard</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai6">Ollama에 Huggingface에서 다운로드 받은 모델 올리기</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai7">Local LLM UI 사용하기 (Open-WebUI)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai8">Alpaca 프롬프트 형식</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai9">Supervised Learning &amp; Unsupervised Learning &amp; Reinforcement Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/ai/ai10">🎧 Whitepaper Companion Podcast 요약: Foundational LLMs &amp; Text Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai11">🎧 Whitepaper Companion Podcast 요약: Prompt Engineering for Kaggle</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai12">🧠 Whitepaper Companion Podcast 요약: Embeddings &amp; Vector Stores</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai13">🎙️ Whitepaper Companion Podcast - Agents | 5-Day Gen AI Intensive Course with Google</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai14">🎧 Agents Companion Whitepaper Podcast Summary</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai15">🧠 도메인 특화 LLM: SecLM &amp; MedLM 상세 분석</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai16">🚀 Vertex AI로 생성형 AI를 운영하는 방법: MLOps 관점에서 본 백서 해설</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai17">📘 Day 2 정리: Embeddings와 벡터 데이터베이스 완전 정복</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai18">📘 Day 3: AI Agents – Generative AI Intensive with Google &amp; Kaggle</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai19">Day 4 요약: 도메인 특화 LLM의 구축과 활용</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/ai/ai20">MLOps for Generative AI: Vertex AI를 활용한 생산 환경 구축 전략</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/llm/llm1">LLM</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">AI</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">🎧 Whitepaper Companion Podcast 요약: Foundational LLMs &amp; Text Generation</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>🎧 Whitepaper Companion Podcast 요약: Foundational LLMs &amp; Text Generation</h1><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="-목차">📑 목차<a href="#-목차" class="hash-link" aria-label="Direct link to 📑 목차" title="Direct link to 📑 목차">​</a></h2><ol><li><a href="#%EC%86%8C%EA%B0%9C-%EB%B0%8F-%EB%AA%A9%ED%91%9C">소개 및 목표</a></li><li><a href="#transformer-%EA%B5%AC%EC%A1%B0-%EC%86%8C%EA%B0%9C">Transformer 구조 소개</a></li><li><a href="#transformer-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%83%81%EC%84%B8-%EC%84%A4%EB%AA%85">Transformer 구성요소 상세 설명</a></li><li><a href="#decoder-only-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EB%93%B1%EC%9E%A5%EA%B3%BC-%EC%9E%A5%EC%A0%90">Decoder-only 모델의 등장과 장점</a></li><li><a href="#mixture-of-experts-moe">Mixture of Experts (MoE)</a></li><li><a href="#llm-%EB%B0%9C%EC%A0%84-%EC%97%B0%EB%8C%80%EA%B8%B0">LLM 발전 연대기</a></li><li><a href="#%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D%EA%B3%BC-%EC%9D%B8%EA%B0%84-%ED%94%BC%EB%93%9C%EB%B0%B1">파인튜닝과 인간 피드백</a></li><li><a href="#parameter-efficient-fine-tuning">Parameter-Efficient Fine-Tuning</a></li><li><a href="#%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81--%EC%83%98%ED%94%8C%EB%A7%81-%EA%B8%B0%EB%B2%95">프롬프트 엔지니어링 &amp; 샘플링 기법</a></li><li><a href="#llm-%ED%8F%89%EA%B0%80-%EB%B0%A9%EB%B2%95">LLM 평가 방법</a></li><li><a href="#%EC%B6%94%EB%A1%A0-%EC%86%8D%EB%8F%84-%EC%B5%9C%EC%A0%81%ED%99%94-%EA%B8%B0%EB%B2%95">추론 속도 최적화 기법</a></li><li><a href="#%EC%8B%A4%EC%A0%9C-%ED%99%9C%EC%9A%A9-%EC%82%AC%EB%A1%80">실제 활용 사례</a></li><li><a href="#%EB%A7%88%EB%AC%B4%EB%A6%AC-%EB%B0%8F-%EB%AF%B8%EB%9E%98-%EC%A0%84%EB%A7%9D">마무리 및 미래 전망</a></li></ol><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="소개-및-목표">소개 및 목표<a href="#소개-및-목표" class="hash-link" aria-label="Direct link to 소개 및 목표" title="Direct link to 소개 및 목표">​</a></h2><ul><li>본 팟캐스트는 <strong>LLM이 어떻게 구성되고 작동하는지</strong>, <strong>어떻게 발전해왔는지</strong>, <strong>어떻게 평가하고 효율화하는지</strong>를 다룬다.</li><li>Transformer 구조부터 최신 모델들까지의 진화를 설명하며, 실제 활용 사례와 미래 전망도 논의한다.</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer-구조-소개">Transformer 구조 소개<a href="#transformer-구조-소개" class="hash-link" aria-label="Direct link to Transformer 구조 소개" title="Direct link to Transformer 구조 소개">​</a></h2><ul><li>Transformer는 2017년 구글의 <strong>번역 프로젝트</strong>에서 시작되었다.</li><li>초기 구조는 <strong>Encoder + Decoder</strong> 형태였다.<ul><li><strong>Encoder</strong>: 입력 문장 (예: 프랑스어)을 벡터로 요약.</li><li><strong>Decoder</strong>: 그 벡터를 바탕으로 출력 문장 (예: 영어)을 생성.</li></ul></li><li>문장은 <strong>토큰(token)</strong> 단위로 처리되며, 이는 단어 하나 또는 단어의 일부일 수 있다.</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer-구성요소-상세-설명">Transformer 구성요소 상세 설명<a href="#transformer-구성요소-상세-설명" class="hash-link" aria-label="Direct link to Transformer 구성요소 상세 설명" title="Direct link to Transformer 구성요소 상세 설명">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-토크나이징-및-임베딩">1. <strong>토크나이징 및 임베딩</strong><a href="#1-토크나이징-및-임베딩" class="hash-link" aria-label="Direct link to 1-토크나이징-및-임베딩" title="Direct link to 1-토크나이징-및-임베딩">​</a></h3><ul><li>문장을 미리 정해진 <strong>vocabulary</strong>에 따라 <strong>토큰</strong>으로 나눈다.</li><li>각 토큰은 의미를 담은 <strong>임베딩 벡터</strong>로 변환된다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-positional-encoding">2. <strong>Positional Encoding</strong><a href="#2-positional-encoding" class="hash-link" aria-label="Direct link to 2-positional-encoding" title="Direct link to 2-positional-encoding">​</a></h3><ul><li>Transformer는 입력을 <strong>동시에 병렬 처리</strong>하므로, 단어 순서를 알 수 없음.</li><li>그래서 <strong>Positional Encoding</strong>을 추가:<ul><li><strong>Sinusoidal 방식</strong> 또는 <strong>학습된 방식(learned)</strong> 존재.</li><li>문장 구조 이해에 영향을 줄 수 있음.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-multi-head-self-attention">3. <strong>Multi-Head Self Attention</strong><a href="#3-multi-head-self-attention" class="hash-link" aria-label="Direct link to 3-multi-head-self-attention" title="Direct link to 3-multi-head-self-attention">​</a></h3><ul><li>문장 내 단어들 간의 관계를 파악하기 위한 핵심 메커니즘.</li><li>예시 문장:  <blockquote><p>“The tiger jumped out of a tree to get a drink because it was thirsty.”<br>
<!-- -->여기서 “it”이 “tiger”를 의미한다는 것을 <strong>self-attention</strong>이 파악.</p></blockquote></li><li>각 단어는 <strong>Query, Key, Value</strong> 벡터를 갖는다.<ul><li>Query: &quot;나는 누구와 관련 있지?&quot;</li><li>Key: 단어의 정체성 정보</li><li>Value: 실제 정보  </li></ul></li><li>Query와 Key를 내적하여 <strong>attention score</strong> 계산 → softmax → 각 단어가 다른 단어에 얼마만큼 집중할지 결정.</li><li>이 모든 연산은 <strong>행렬 단위로 병렬 처리</strong>된다.</li><li><strong>Multi-head</strong> 구조로 각 head는 서로 다른 관계(문법적, 의미적 등)를 학습한다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-layer-normalization--residual-connection">4. <strong>Layer Normalization &amp; Residual Connection</strong><a href="#4-layer-normalization--residual-connection" class="hash-link" aria-label="Direct link to 4-layer-normalization--residual-connection" title="Direct link to 4-layer-normalization--residual-connection">​</a></h3><ul><li>LayerNorm: 각 층의 활성값을 안정적으로 유지</li><li>Residual: 입력을 다음 출력에 더해줌 → <strong>정보 손실 방지</strong> + <strong>학습 안정화</strong></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-feed-forward-layer">5. <strong>Feed Forward Layer</strong><a href="#5-feed-forward-layer" class="hash-link" aria-label="Direct link to 5-feed-forward-layer" title="Direct link to 5-feed-forward-layer">​</a></h3><ul><li>각 토큰에 대해 독립적으로 적용되는 2개의 선형 변환 + 비선형 활성함수 (ReLU, GeLU 등)</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="decoder-only-모델의-등장과-장점">Decoder-only 모델의 등장과 장점<a href="#decoder-only-모델의-등장과-장점" class="hash-link" aria-label="Direct link to Decoder-only 모델의 등장과 장점" title="Direct link to Decoder-only 모델의 등장과 장점">​</a></h2><ul><li>텍스트 생성에 집중한 모델들은 <strong>Decoder-only 구조</strong>를 채택 (예: GPT 계열).</li><li><strong>Masked Self-Attention</strong> 사용:<ul><li>다음 토큰을 예측할 때 이전 토큰만 참고할 수 있게 제한함 → 사람이 글 쓰는 방식과 유사.</li></ul></li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="mixture-of-experts-moe">Mixture of Experts (MoE)<a href="#mixture-of-experts-moe" class="hash-link" aria-label="Direct link to Mixture of Experts (MoE)" title="Direct link to Mixture of Experts (MoE)">​</a></h2><ul><li>초대형 모델의 효율성을 높이기 위한 방식.</li><li>여러 <strong>전문 sub-model (expert)</strong>을 구성하고, 입력마다 일부만 활성화.</li><li><strong>Gating network</strong>가 어떤 expert를 쓸지 결정.</li><li>성능은 유지하면서도 연산량을 줄임.</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-발전-연대기">LLM 발전 연대기<a href="#llm-발전-연대기" class="hash-link" aria-label="Direct link to LLM 발전 연대기" title="Direct link to LLM 발전 연대기">​</a></h2><table><thead><tr><th>연도</th><th>모델</th><th>특징</th></tr></thead><tbody><tr><td>2018</td><td>GPT-1</td><td>Decoder-only, BookCorpus, 반복 문제</td></tr><tr><td>2018</td><td>BERT</td><td>Encoder-only, 언어 이해에 특화</td></tr><tr><td>2019</td><td>GPT-2</td><td>WebText, zero-shot 가능</td></tr><tr><td>2020+</td><td>GPT-3</td><td>175B 파라미터, few-shot 가능</td></tr><tr><td>이후</td><td>GPT-3.5, GPT-4</td><td>코드 이해, 멀티모달 기능</td></tr><tr><td>2021</td><td>LaMDA (Google)</td><td>대화에 특화</td></tr><tr><td>2021</td><td>Gopher (DeepMind)</td><td>고품질 데이터, 지식 기반 태스크</td></tr><tr><td>2022</td><td>Chinchilla (DeepMind)</td><td>파라미터 대비 학습 데이터 증가</td></tr><tr><td>2022</td><td>PaLM</td><td>Google Pathways, 확장성 개선</td></tr><tr><td>2023</td><td>PaLM 2</td><td>적은 파라미터로 더 나은 성능</td></tr><tr><td>2024</td><td>Gemini</td><td>멀티모달, 대용량 컨텍스트, MoE</td></tr><tr><td>2024</td><td>Gemma 1, 2</td><td>경량, 오픈소스, LLaMA 3보다 빠름</td></tr><tr><td>-</td><td>LLaMA 1/2/3</td><td>Meta, 다국어, 비전 모델 포함</td></tr><tr><td>-</td><td>Mistral Mixtral</td><td>Sparse MoE, 수학 및 다국어 우수</td></tr><tr><td>-</td><td>DeepSeek R1</td><td>RL 기법, OpenAI 01 수준</td></tr><tr><td>-</td><td>기타</td><td>Qwen, Yi, Grok 등 다양한 오픈모델</td></tr></tbody></table><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="파인튜닝과-인간-피드백">파인튜닝과 인간 피드백<a href="#파인튜닝과-인간-피드백" class="hash-link" aria-label="Direct link to 파인튜닝과 인간 피드백" title="Direct link to 파인튜닝과 인간 피드백">​</a></h2><ul><li><strong>Pretraining</strong>: 라벨 없는 방대한 텍스트로 기본 언어 능력 학습.</li><li><strong>Fine-tuning</strong>: 특정 태스크용 라벨된 소규모 데이터로 특화.</li><li><strong>SFT (Supervised Fine-Tuning)</strong>:<ul><li>Prompt-Response 쌍으로 학습 → 행동 패턴 및 안전성 강화</li></ul></li><li><strong>RLHF (Reinforcement Learning from Human Feedback)</strong>:<ul><li>사람의 선호를 학습한 <strong>Reward Model</strong>을 이용하여 LLM을 강화학습</li></ul></li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="parameter-efficient-fine-tuning">Parameter-Efficient Fine-Tuning<a href="#parameter-efficient-fine-tuning" class="hash-link" aria-label="Direct link to Parameter-Efficient Fine-Tuning" title="Direct link to Parameter-Efficient Fine-Tuning">​</a></h2><ul><li><strong>전체 모델 파인튜닝은 비싸다</strong> → 일부 파라미터만 조정하는 기법들 등장</li></ul><table><thead><tr><th>기법</th><th>설명</th></tr></thead><tbody><tr><td>Adapter</td><td>중간 층에 작은 모듈 삽입하여 거기만 학습</td></tr><tr><td>LoRA</td><td>weight 변화를 저랭크 행렬로 근사</td></tr><tr><td>QLoRA</td><td>LoRA + 양자화(quantization)</td></tr><tr><td>Soft Prompting</td><td>입력 앞에 학습된 벡터 추가</td></tr></tbody></table><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="프롬프트-엔지니어링--샘플링-기법">프롬프트 엔지니어링 &amp; 샘플링 기법<a href="#프롬프트-엔지니어링--샘플링-기법" class="hash-link" aria-label="Direct link to 프롬프트 엔지니어링 &amp; 샘플링 기법" title="Direct link to 프롬프트 엔지니어링 &amp; 샘플링 기법">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-프롬프트-기법">🧠 프롬프트 기법<a href="#-프롬프트-기법" class="hash-link" aria-label="Direct link to 🧠 프롬프트 기법" title="Direct link to 🧠 프롬프트 기법">​</a></h3><ul><li><strong>Zero-shot</strong>: 예시 없이 직접 질문</li><li><strong>Few-shot</strong>: 예시 몇 개 제공</li><li><strong>Chain-of-Thought</strong>: 추론 과정을 단계별로 보여줌</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-샘플링-기법">🎲 샘플링 기법<a href="#-샘플링-기법" class="hash-link" aria-label="Direct link to 🎲 샘플링 기법" title="Direct link to 🎲 샘플링 기법">​</a></h3><table><thead><tr><th>기법</th><th>특징</th></tr></thead><tbody><tr><td>Greedy</td><td>항상 가장 확률 높은 토큰 선택 → 반복 가능성 ↑</td></tr><tr><td>Random Sampling</td><td>창의성 ↑, 오류 가능성 ↑</td></tr><tr><td>Temperature</td><td>무작위성 조절 (↑면 창의성 ↑)</td></tr><tr><td>Top-k</td><td>확률 높은 k개 중에서 샘플링</td></tr><tr><td>Top-p (Nucleus)</td><td>누적 확률이 p 이하인 토큰 중에서 선택</td></tr><tr><td>Best-of-n</td><td>여러 개 생성 후 가장 좋은 것 선택</td></tr></tbody></table><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-평가-방법">LLM 평가 방법<a href="#llm-평가-방법" class="hash-link" aria-label="Direct link to LLM 평가 방법" title="Direct link to LLM 평가 방법">​</a></h2><ul><li><strong>정량 평가</strong>: BLEU, ROUGE 등 → 정답에 얼마나 가까운지</li><li><strong>정성 평가</strong>: 사람 리뷰어가 유창성, 일관성, 창의성 등 평가</li><li><strong>AI 평가자</strong>: LLM이 다른 LLM의 출력을 채점 (Generative, Discriminative, Reward 모델 등)</li><li><strong>다중 기준 평가</strong>: 정확도, 유용성, 스타일 등 맞춤 평가 기준 설정</li><li><strong>멀티모달 평가</strong>: 텍스트, 이미지, 영상 각각 따로 평가 필요</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="추론-속도-최적화-기법">추론 속도 최적화 기법<a href="#추론-속도-최적화-기법" class="hash-link" aria-label="Direct link to 추론 속도 최적화 기법" title="Direct link to 추론 속도 최적화 기법">​</a></h2><table><thead><tr><th>범주</th><th>기법</th><th>설명</th></tr></thead><tbody><tr><td>🔄 출력 변화 있음</td><td>Quantization</td><td>연산을 8비트/4비트로 줄여 속도 증가</td></tr><tr><td></td><td>Distillation</td><td>작은 모델이 큰 모델 모방</td></tr><tr><td>✅ 출력 유지</td><td>Flash Attention</td><td>데이터 이동 최소화, 병렬화 향상</td></tr><tr><td></td><td>Prefix Caching</td><td>대화 컨텍스트 부분 결과 캐싱</td></tr><tr><td></td><td>Speculative Decoding</td><td>빠른 Drafter 모델 예측 → 검증 후 건너뜀</td></tr><tr><td>병렬화</td><td>Batching / Parallelization</td><td>다수 요청 동시 처리 or 연산 분산</td></tr></tbody></table><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="실제-활용-사례">실제 활용 사례<a href="#실제-활용-사례" class="hash-link" aria-label="Direct link to 실제 활용 사례" title="Direct link to 실제 활용 사례">​</a></h2><ul><li><strong>코딩</strong>: 생성, 리팩토링, 디버깅, 문서화, 코드 번역 등 (예: AlphaCode 2)</li><li><strong>수학/과학</strong>: FundSearch, AlphaGeometry → 수학적 발견 보조</li><li><strong>번역/요약/질문응답</strong>: 자연스럽고 정확한 결과 제공</li><li><strong>대화/창작</strong>: 광고, 대본, 창의적 글쓰기 등</li><li><strong>의료/법률/감정 분석</strong>: 진단 보조, 문서 분석, 고객 피드백 분석</li><li><strong>LLM 평가자 역할</strong>: 다른 모델의 출력을 채점</li><li><strong>데이터 분석</strong>: 대규모 데이터에서 인사이트 추출</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="마무리-및-미래-전망">마무리 및 미래 전망<a href="#마무리-및-미래-전망" class="hash-link" aria-label="Direct link to 마무리 및 미래 전망" title="Direct link to 마무리 및 미래 전망">​</a></h2><ul><li>Transformer 기반 구조는 여전히 핵심.</li><li>LLM은 빠르게 발전 중이며, <strong>멀티모달 기능</strong>과 <strong>대규모 컨텍스트 윈도우</strong>가 핵심 경쟁 요소.</li><li><strong>다양한 응용 분야</strong>에서 LLM의 활용은 계속 확장될 것으로 기대됨.</li><li>다음 세대 LLM으로 무엇이 가능해질지, 어떤 과제가 있는지에 대한 청취자 의견 요청으로 마무리.</li></ul><hr></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/ai/ai9"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Supervised Learning &amp; Unsupervised Learning &amp; Reinforcement Learning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/ai/ai11"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">🎧 Whitepaper Companion Podcast 요약: Prompt Engineering for Kaggle</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#-목차" class="table-of-contents__link toc-highlight">📑 목차</a></li><li><a href="#소개-및-목표" class="table-of-contents__link toc-highlight">소개 및 목표</a></li><li><a href="#transformer-구조-소개" class="table-of-contents__link toc-highlight">Transformer 구조 소개</a></li><li><a href="#transformer-구성요소-상세-설명" class="table-of-contents__link toc-highlight">Transformer 구성요소 상세 설명</a><ul><li><a href="#1-토크나이징-및-임베딩" class="table-of-contents__link toc-highlight">1. <strong>토크나이징 및 임베딩</strong></a></li><li><a href="#2-positional-encoding" class="table-of-contents__link toc-highlight">2. <strong>Positional Encoding</strong></a></li><li><a href="#3-multi-head-self-attention" class="table-of-contents__link toc-highlight">3. <strong>Multi-Head Self Attention</strong></a></li><li><a href="#4-layer-normalization--residual-connection" class="table-of-contents__link toc-highlight">4. <strong>Layer Normalization &amp; Residual Connection</strong></a></li><li><a href="#5-feed-forward-layer" class="table-of-contents__link toc-highlight">5. <strong>Feed Forward Layer</strong></a></li></ul></li><li><a href="#decoder-only-모델의-등장과-장점" class="table-of-contents__link toc-highlight">Decoder-only 모델의 등장과 장점</a></li><li><a href="#mixture-of-experts-moe" class="table-of-contents__link toc-highlight">Mixture of Experts (MoE)</a></li><li><a href="#llm-발전-연대기" class="table-of-contents__link toc-highlight">LLM 발전 연대기</a></li><li><a href="#파인튜닝과-인간-피드백" class="table-of-contents__link toc-highlight">파인튜닝과 인간 피드백</a></li><li><a href="#parameter-efficient-fine-tuning" class="table-of-contents__link toc-highlight">Parameter-Efficient Fine-Tuning</a></li><li><a href="#프롬프트-엔지니어링--샘플링-기법" class="table-of-contents__link toc-highlight">프롬프트 엔지니어링 &amp; 샘플링 기법</a><ul><li><a href="#-프롬프트-기법" class="table-of-contents__link toc-highlight">🧠 프롬프트 기법</a></li><li><a href="#-샘플링-기법" class="table-of-contents__link toc-highlight">🎲 샘플링 기법</a></li></ul></li><li><a href="#llm-평가-방법" class="table-of-contents__link toc-highlight">LLM 평가 방법</a></li><li><a href="#추론-속도-최적화-기법" class="table-of-contents__link toc-highlight">추론 속도 최적화 기법</a></li><li><a href="#실제-활용-사례" class="table-of-contents__link toc-highlight">실제 활용 사례</a></li><li><a href="#마무리-및-미래-전망" class="table-of-contents__link toc-highlight">마무리 및 미래 전망</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.cca82d27.js"></script>
<script src="/assets/js/main.fe143d0c.js"></script>
</body>
</html>