<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-llm/llm1">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">[LLM] 1. LLM 개요 및 기본 개념 | A Little &quot;Bit&quot;</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://kihyun1998.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://kihyun1998.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://kihyun1998.github.io/docs/llm/llm1"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="[LLM] 1. LLM 개요 및 기본 개념 | A Little &quot;Bit&quot;"><meta data-rh="true" name="description" content="---"><meta data-rh="true" property="og:description" content="---"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://kihyun1998.github.io/docs/llm/llm1"><link data-rh="true" rel="alternate" href="https://kihyun1998.github.io/docs/llm/llm1" hreflang="en"><link data-rh="true" rel="alternate" href="https://kihyun1998.github.io/docs/llm/llm1" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="A Little &quot;Bit&quot; RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="A Little &quot;Bit&quot; Atom Feed"><link rel="stylesheet" href="/assets/css/styles.3b1b8064.css">
<link rel="preload" href="/assets/js/runtime~main.841f889b.js" as="script">
<link rel="preload" href="/assets/js/main.f675741a.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/android-icon-48x48.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/android-icon-48x48.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">A Little &quot;Bit&quot;</b></a><a class="navbar__item navbar__link" href="/docs/blog-intro">블로그 배포하기</a><a class="navbar__item navbar__link" href="/docs/design-pattern-intro">개인 공부</a><a class="navbar__item navbar__link" href="/docs/react-intro">JavaScript</a><a class="navbar__item navbar__link" href="/docs/dart-intro">Dart &amp; Flutter</a><a class="navbar__item navbar__link" href="/docs/go/go1">Go</a><a class="navbar__item navbar__link" href="/docs/backend-basic/backend1">BackEnd</a><a class="navbar__item navbar__link" href="/docs/algorithm/al1">Algorithm</a><a class="navbar__item navbar__link" href="/docs/error/error1">Error</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/ai/ai1">AI</a><a class="navbar__item navbar__link" href="/docs/oracle-cloud/oracle-cloud1">Infra</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/ai/ai1">AI</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/llm/llm1">LLM</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/llm/llm1">[LLM] 1. LLM 개요 및 기본 개념</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm2">[LLM] 2. Transformer 아키텍처 및 기본 개념</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm3">[LLM] 3. GPT 모델 개요 및 실습</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm4">[LLM] 4. LLM이 이해하는 text</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm5">[LLM] 5. Position Encoding 실습</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm6">[LLM] 6. WordPiece와 BPE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm7">[LLM] 7. 슬라이딩 윈도우</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm8">[LLM] 8. Token Embedding</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm9">[LLM] 9. Self Attention</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm10">[LLM] 10. Casual Attention</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm11">[LLM] 11. Multi-Head Attention</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm12">[LLM] 12. Transformer Block 이해하기</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm13">[LLM] 13. Transformer Block 실습</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm14">[LLM] 14. Transformer Block 연결하기</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm15">[LLM] 15. 토큰 입력 부터 단어 예측까지</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm16">[LLM] 16. 학습 평가</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm17">[LLM] 17. 사전 학습</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm18">[LLM] 18. 사전학습 용어 정리</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/llm/llm19">[LLM] 19. 파인 튜닝 개요</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">LLM</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[LLM] 1. LLM 개요 및 기본 개념</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>[LLM]<!-- --> 1. LLM 개요 및 기본 개념</h1><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm">LLM<a href="#llm" class="hash-link" aria-label="Direct link to LLM" title="Direct link to LLM">​</a></h2><hr><h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm이란">LLM이란<a href="#llm이란" class="hash-link" aria-label="Direct link to LLM이란" title="Direct link to LLM이란">​</a></h3><p>LLM은 Large Language Model의 약자로 방대한 양의 텍스트 데이터를 학습해서 언어 이해 및 생성 능력을 갖춘 인공지능 모델을 뜻합니다.</p><p>대표적으로 <code>GPT</code>, <code>Llama</code>, <code>Claude</code> 등이 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="특징">특징<a href="#특징" class="hash-link" aria-label="Direct link to 특징" title="Direct link to 특징">​</a></h3><ul><li>✅ 대규모 데이터 학습 -&gt; 수십억 개의 단어를 학습한다.</li><li>✅ 문맥 이해 능력 -&gt; 이전 문장을 기반으로 의미 있는 텍스트를 생성한다.</li><li>✅ 다양한 응용 분야 -&gt; 챗봇, 번역, 문서 요약, 코드 생성 등.. 다양한 분야에서 활용할 수 있다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm의-주요-응용-분야">LLM의 주요 응용 분야<a href="#llm의-주요-응용-분야" class="hash-link" aria-label="Direct link to LLM의 주요 응용 분야" title="Direct link to LLM의 주요 응용 분야">​</a></h3><table><thead><tr><th>📌 분야</th><th>🛠 활용 예시</th></tr></thead><tbody><tr><td><strong>번역</strong></td><td>Google Translate, DeepL</td></tr><tr><td><strong>챗봇</strong></td><td>ChatGPT, Claude, Google Gemini</td></tr><tr><td><strong>코드 생성</strong></td><td>GitHub Copilot, OpenAI Codex</td></tr><tr><td><strong>문서 요약</strong></td><td>뉴스 요약, 논문 요약</td></tr><tr><td><strong>검색 및 질의 응답</strong></td><td>Google Bard, Bing AI</td></tr><tr><td><strong>창작 및 글쓰기 보조</strong></td><td>AI 기반 스토리 생성, 시나리오 작성</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm-개발-프로세스">LLM 개발 프로세스<a href="#llm-개발-프로세스" class="hash-link" aria-label="Direct link to LLM 개발 프로세스" title="Direct link to LLM 개발 프로세스">​</a></h3><p>LLM을 만들려면 다음 세가지 단계가 필요합니다.</p><ol><li><strong>데이터 준비</strong> : 텍스트 데이터 수집 및 전처리 (토큰화,샘플링)  </li><li><strong>모델 학습</strong> : Transformer 기반 모델을 사용하여 학습</li><li><strong>배포 및 활용</strong> : API로 배포 및 app에 통합</li></ol><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_S0QG"><p>데이터 &gt; 전처리 &gt; 학습(Pretraining) &gt; 미세조정(Fine-tuning) &gt; 배포 및 활용</p></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer-아키텍처">Transformer 아키텍처<a href="#transformer-아키텍처" class="hash-link" aria-label="Direct link to Transformer 아키텍처" title="Direct link to Transformer 아키텍처">​</a></h2><hr><p>LLM은 Transformer라는 신경망 구조를 기반으로 합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="transformer의-주요-개념">Transformer의 주요 개념<a href="#transformer의-주요-개념" class="hash-link" aria-label="Direct link to Transformer의 주요 개념" title="Direct link to Transformer의 주요 개념">​</a></h3><ul><li>✅ Self-Attention: 문장에서 중요한 단어를 찾아 가중치를 부여</li><li>✅ Multi-Head Attention: 여러 시각으로 문맥을 분석</li><li>✅ Feed Forward Network: 각 토큰을 독립적으로 변환</li><li>✅ Layer Normalization &amp; Residual Connection: 학습 안전화</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="self-attention">Self Attention<a href="#self-attention" class="hash-link" aria-label="Direct link to Self Attention" title="Direct link to Self Attention">​</a></h3><p>Transfromer의 핵심 메커니즘으로 입력 문장에서 중요한 단어를 찾아서 가중치를 부여하는 역할을 합니다.</p><ul><li>문장 내에서 각 단어가 다른 단어와 얼마나 관련이 있는지를 계산하여 가중치를 부여합니다.</li></ul><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>계산 방식</div><div class="admonitionContent_S0QG"><ol><li>각 단어를 <code>Query(Q)</code>, <code>Key(K)</code>, <code>Value(V)</code> 벡터로 변환합니다.  </li><li>Query와 Key 간의 <strong>dot product</strong>를 계산하여 유사도를 측정합니다.  </li><li><strong>softmax</strong> 함수를 적용하여 어텐션 가중치를 생성합니다.  </li><li>이 가중치를 Value 백터에 적용하여 최종 <strong>Context Vector</strong>를 생성합니다.</li></ol></div></div><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>예시</div><div class="admonitionContent_S0QG"><p>문장 <code>&quot;I love deep learning.&quot;</code>이 있다고 하자.  </p><ul><li>&quot;deep&quot; 단어는 &quot;learning&quot;과 강한 연관성이 있다. 그래서 높은 가중치를 부여한다.  </li><li>&quot;I&quot; 단어는 &quot;learning&quot;과 덜 연관돼있다. 그래서 낮은 가중치를 부여한다.</li></ul></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-head-attention">Multi-Head Attention<a href="#multi-head-attention" class="hash-link" aria-label="Direct link to Multi-Head Attention" title="Direct link to Multi-Head Attention">​</a></h3><p>Self-Attention을 여러 개 실행하여 문장을 다양한 관점에서 분석하는 기법입니다.</p><ul><li><p>✅ 필요한 이유</p><ul><li>Self-Attention 만으로는 하나의 문맥 패턴만 학습할 수 있습니다.</li><li>Multi-Head Attention을 사용하면 문장에서 다양한 의미와 구조 관계를 포착할 수 있습니다.</li></ul></li><li><p>✅ 구현 방식</p><ol><li>여러 개의 Self-Attention 모듈을 병렬 실행합니다.  </li><li>각각의 결과를 연결한 후 다시 가중치를 적용하여 최종 결과를 생성합니다.</li></ol></li></ul><p>한 가지 시각(Self-Attention)으로 본다면 제한적인 정보를 얻지만 여러 시각(Multi-Head Attention)으로 본다면 더 풍부한 정보를 얻는 것과 같습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="feed-forward-network-ffn">Feed Forward Network (FFN)<a href="#feed-forward-network-ffn" class="hash-link" aria-label="Direct link to Feed Forward Network (FFN)" title="Direct link to Feed Forward Network (FFN)">​</a></h3><p>각 토큰을 독립적으로 변환하는 과정입니다.</p><ul><li><p>✅ 구조</p><ol><li>입력 백터 -&gt; 선형 변환 (W1)  </li><li>비선형 활성화 함수 (GELU or ReLU)  </li><li>출력 백터 -&gt; 또 다른 선형 변환 (W2)</li></ol></li><li><p>✅ 중요한 이유</p><ul><li>Attention 과정에서 학습한 정보를 변형하여 더욱 풍부한 표현으로 만들도록 돕습니다.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="layer-normalization--residual-connection">Layer Normalization &amp; Residual Connection<a href="#layer-normalization--residual-connection" class="hash-link" aria-label="Direct link to Layer Normalization &amp; Residual Connection" title="Direct link to Layer Normalization &amp; Residual Connection">​</a></h3><p>모델 학습 안정성을 높이는 기법입니다.</p><ul><li><p>✅ Layer Normalization(레이어 정규화)</p><ul><li>각 레이어의 출력값을 정규화하여 학습 안정성을 증가시킵니다.</li><li>변환된 벡터의 평균을 0, 분산을 1로 맞추어 수렴 속도를 향상시킵니다.</li></ul></li><li><p>✅ Residual Connection (잔차 연결)</p><ul><li>입력값을 변형 없이 그대로 다음 레이어로 전달하는 구조입니다.</li><li>기울기 소실 문제를 완화하여 깊은 신경망에서도 효과적인 학습이 가능합니다.</li></ul></li></ul><p>Residual Connection은 학습 과정에서 중요한 정보가 손실되지 않도록 원본 데이터를 보존하는 역할을 합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="encoder--decoder">Encoder &amp; Decoder<a href="#encoder--decoder" class="hash-link" aria-label="Direct link to Encoder &amp; Decoder" title="Direct link to Encoder &amp; Decoder">​</a></h3><ul><li><p>✅ Encoder</p><ul><li>Encoder는 입력 문장을 받아서 그 문장의 의미를 압축하여 벡터 표현으로 변환하는 역할을 합니다.</li><li>여러 개의 Self-Attention 레이어를 거치면서 문장 내 각 단어의 의미를 학습합니다.</li><li>보통 문장 분석, 감성, 문서 분류 등 문장의 의미를 이해하는 작업에 사용됩니다.</li></ul></li><li><p>✅ Decoder</p><ul><li>입력 문장과 Encoder가 만든 벡터 표현을 이용해서 새로운 문장을 생성하는 역할을 합니다.</li><li>일반적으로 이전 단어들을 참고하여 다음 단어들을 예측하는 방식으로 동작합니다.</li><li>기계 번역, 텍스트 생성, 챗봇 등 새로운 문장을 만들어야하는 작업에 사용됩니다.</li></ul></li></ul><p><code>Encoder</code>는 사람의 뇌에서 입력 문장을 분석하는 과정. like &quot;이 문장은 긍적적이네?&quot;<br>
<code>Decoder</code>는 분석한 내용을 바탕으로 새로운 문장을 생성하는 과정. like &quot;그럼 긍정적인 답변을 만들어야지.&quot;</p><table><thead><tr><th></th><th><strong>BERT</strong></th><th><strong>GPT</strong></th></tr></thead><tbody><tr><td><strong>사용하는 Transformer 부분</strong></td><td><strong>Encoder</strong></td><td><strong>Decoder</strong></td></tr><tr><td><strong>훈련 방식</strong></td><td>양방향(Bidirectional)</td><td>단방향(Autoregressive)</td></tr><tr><td><strong>입력 데이터 처리 방식</strong></td><td>문장 전체를 동시에 고려</td><td>앞의 단어들만 보고 다음 단어를 예측</td></tr><tr><td><strong>주요 활용 분야</strong></td><td>감성 분석, 문서 분류, 질문 응답</td><td>텍스트 생성, 대화 모델, 문장 자동 완성</td></tr><tr><td><strong>훈련 목표</strong></td><td>Masked Language Modeling (MLM)</td><td>Causal Language Modeling (CLM)</td></tr><tr><td><strong>대표적인 응용 모델</strong></td><td>Google Search, BERT 기반 QA 시스템</td><td>ChatGPT, GPT-4</td></tr></tbody></table></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/ai/ai20"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">MLOps for Generative AI: Vertex AI를 활용한 생산 환경 구축 전략</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/llm/llm2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[LLM] 2. Transformer 아키텍처 및 기본 개념</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#llm" class="table-of-contents__link toc-highlight">LLM</a><ul><li><a href="#llm이란" class="table-of-contents__link toc-highlight">LLM이란</a></li><li><a href="#특징" class="table-of-contents__link toc-highlight">특징</a></li><li><a href="#llm의-주요-응용-분야" class="table-of-contents__link toc-highlight">LLM의 주요 응용 분야</a></li><li><a href="#llm-개발-프로세스" class="table-of-contents__link toc-highlight">LLM 개발 프로세스</a></li></ul></li><li><a href="#transformer-아키텍처" class="table-of-contents__link toc-highlight">Transformer 아키텍처</a><ul><li><a href="#transformer의-주요-개념" class="table-of-contents__link toc-highlight">Transformer의 주요 개념</a></li><li><a href="#self-attention" class="table-of-contents__link toc-highlight">Self Attention</a></li><li><a href="#multi-head-attention" class="table-of-contents__link toc-highlight">Multi-Head Attention</a></li><li><a href="#feed-forward-network-ffn" class="table-of-contents__link toc-highlight">Feed Forward Network (FFN)</a></li><li><a href="#layer-normalization--residual-connection" class="table-of-contents__link toc-highlight">Layer Normalization &amp; Residual Connection</a></li><li><a href="#encoder--decoder" class="table-of-contents__link toc-highlight">Encoder &amp; Decoder</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.841f889b.js"></script>
<script src="/assets/js/main.f675741a.js"></script>
</body>
</html>