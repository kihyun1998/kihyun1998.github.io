---
sidebar_position: 15
---

# [LLM] 15. í† í° ì…ë ¥ ë¶€í„° ë‹¨ì–´ ì˜ˆì¸¡ê¹Œì§€
---


## í† í° ì„ë² ë”© ì½”ë“œ
---

```python

class GPTModel(nn.Module):
    def __init__(self, vocab_size,d_model,num_heads, num_layers,dropout,context_length):
        super().__init__()

        # 1. Token embedding (ë‹¨ì–´ > ë²¡í„°)
        self.token_embedding = nn.Embedding(vocab_size,d_model)

        # 2. position encoding
        self.position_encoding = nn.Embedding(context_length, d_model)
        
        # 3. transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, dropout, context_length) for _ in range(num_layers)
        ])

        # 4. Linear Projection
        self.ln_final = nn.LayerNorm(d_model)
        self.output_layer = nn.Linear(d_model,vocab_size)
        
    def forward(self, input_ids, mask=None):
        # 1. token embedding ì ìš©
        token_embeds = self.token_embedding(input_ids)

        # 2. position embedding ì ìš©
        pos_embeds = self.position_encoding(torch.arange(input_ids.shape[1],device=input_ids.device))

        # token embedding + position encoding
        x = token_embeds + pos_embeds

        # 4. transformer block í†µê³¼
        for block in self.blocks:
            x = block(x, mask)
        
        # 5. ìµœì¢…
        x = self.ln_final(x)
        logits = self.output_layer(x)

        return logits
```


### ì‹œê°í™” í•´ì„œ í™•ì¸

```python
# ëª¨ë¸ ì„¤ì •
vocab_size = 100    # ë‹¨ì–´ ê°œìˆ˜
d_model = 128       # ì„ë² ë”© ì°¨ì›
context_length = 20 # ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´

# GPT ëª¨ë¸ ìƒì„±
model = GPTModel(vocab_size,d_model,num_heads=4,num_layers=4,dropout=0.1,context_length=context_length)

# ê°€ì§œ ë°ì´í„°
input_ids = torch.randint(0,vocab_size,(1,context_length))

with torch.no_grad():
    token_embeds = model.token_embedding(input_ids).squeeze(0).numpy()  # (seq_length, d_model)
    pos_embeds = model.position_encoding(torch.arange(context_length)).numpy()  # (seq_length, d_model)
    combined_embeds = token_embeds + pos_embeds  # ìµœì¢… Transformer ì…ë ¥

# ğŸ”¹ íˆíŠ¸ë§µ ì‹œê°í™” (Embedding í™•ì¸)
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.heatmap(token_embeds, cmap="coolwarm", ax=axes[0])
axes[0].set_title("Token Embedding")

sns.heatmap(pos_embeds, cmap="coolwarm", ax=axes[1])
axes[1].set_title("Positional Encoding")

sns.heatmap(combined_embeds, cmap="coolwarm", ax=axes[2])
axes[2].set_title("Token + Positional Encoding")

plt.show()
```

![alt text](img/15/1.png)


## ì˜ˆì¸¡í•´ë³´ê¸°
---

```python
import torch
import torch.nn.functional as F

from embedding import GPTModel

# ì–´íœ˜ì§‘
vocabulary = ["hello","world","this","is","a","test","model","GPT","language","AI"]
vocabulary_size = len(vocabulary)

# í† í° ì¸ì½”ë”© í•¨ìˆ˜
def encode(text):
    return torch.tensor([vocabulary.index(word) for word in text.split() if word in vocabulary]).unsqueeze(0)

# í† í° ë””ì½”ë”© í•¨ìˆ˜
def decode(tokens):
    return " ".join([vocabulary[index] for index in tokens])

# ê°€ì§œ ì…ë ¥ ë¬¸ì¥
input_text = "hello world this is"
input_ids = encode(input_text)

# ëª¨ë¸ ìƒì„±
model = GPTModel(
    vocab_size=vocabulary_size, 
    d_model=128,
    num_heads=4,
    num_layers=4,
    dropout=0.1,
    context_length=vocabulary_size)

model.eval() # í‰ê°€ ëª¨ë“œ

with torch.no_grad():
    logits = model(input_ids)
    next_token_logits = logits[:,-1,:]
    next_token = torch.argmax(F.softmax(next_token_logits,dim=1),dim=-1).item()

# ê²°ê³¼ ì¶œë ¥
print(f"ì…ë ¥ ë¬¸ì¥: {input_text}")
print(f"ì˜ˆì¸¡ëœ ë‹¤ìŒ ë‹¨ì–´: {vocabulary[next_token]}")
```

ì•„ë˜ ì²˜ëŸ¼ ì¶œë ¥ë˜ëŠ”ë° ì–´ì°¨í”¼ ëœë¤ì…ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ëª¨ë¸ì„ í†µí•´ í•™ìŠµì´ ëœê²Œ ì•„ë‹ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

```text
ì…ë ¥ ë¬¸ì¥: hello world this is
ì˜ˆì¸¡ëœ ë‹¤ìŒ ë‹¨ì–´: test
```



### ì—¬ëŸ¬ê°œ ì˜ˆì¸¡í•˜ê¸°

```python
def generate(model,tokenizer,start_text,max_new_tokens):
    model.eval()
    input_ids=encode(start_text)

    for _  in range(max_new_tokens):
        with torch.no_grad():
            logits = model(input_ids)
            next_token_logits = logits[:,-1,:]
            next_token = torch.argmax(F.softmax(next_token_logits,dim=1),dim=-1).item()
            
            # ì…ë ¥ì— ì¶”ê°€
            input_ids = torch.cat([input_ids, torch.tensor([[next_token]])], dim=1)

    return decode(input_ids.squeeze(0).tolist())

# ì‹¤í–‰
generated_text = generate(model, encode, "hello world this is", max_new_tokens=5)
print("ğŸ”¥ ìƒì„±ëœ ë¬¸ì¥:", generated_text)
```



```text
ğŸ”¥ ìƒì„±ëœ ë¬¸ì¥: hello world this is a world model AI a
```

ì´ëŸ°ì‹ìœ¼ë¡œ ì—¬ëŸ¬ê°œ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.. í•™ìŠµì´ ëœ ëª¨ë¸ì´ ì•„ë‹ˆê¸°ì— ëœë¤ê°’ì…ë‹ˆë‹¤.




## ì „ì— ë‹¤ë¤˜ë˜ ì½”ë“œ
---

### Transformer Block
```python
class TransformerBlock(nn.Module):
    """
    d_model: ì…ë ¥ ì°¨ì› í¬ê¸°
    num_heads: Multi-Head Attentionì—ì„œ ì‚¬ìš©í•  Attention Head ê°œìˆ˜
    dropout: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    context_length: í•œë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´
    """
    def __init__(self, d_model, num_heads, dropout, context_length):
        super().__init__()

        self.context_length = context_length
        # Layer Normalization (Pre-LayerNorm ë°©ì‹)
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        # Multi-Head Attention (MHA)
        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)

        # Feed Forward Network (FFN)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),  # í™•ì¥ (ê¸°ë³¸ì ìœ¼ë¡œ 4ë°° í¬ê¸°ë¡œ ì¦ê°€)
            nn.GELU(),  # í™œì„±í™” í•¨ìˆ˜
            nn.Linear(d_model * 4, d_model),  # ë‹¤ì‹œ ì›ë˜ í¬ê¸°ë¡œ ì¶•ì†Œ
            nn.Dropout(dropout),
        )

        # ë¯¸ë¦¬ causal maskë¥¼ ë§Œë“¤ì–´ë‘ê¸°
        self.register_buffer("causal_mask", self._create_causal_mask(context_length))

    def _create_causal_mask(self, seq_length):
        """
        ìƒì‚¼ê° í–‰ë ¬ì„ ë§Œë“¤ì–´ì„œ ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ë„ë¡ í•¨.
        """
        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask

    def forward(self, x, mask=None):
        # ì…ë ¥ ì‹œí€€ìŠ¤ í™•ì¸
        seq_length = x.size(1)
        if mask is None:
            mask = self._create_causal_mask(seq_length).to(x.device) 

        # Multi-Head Attention (Self-Attention)
        x = self.ln1(x)
        attn_out,_ = self.attn(x,x,x,attn_mask=mask,need_weights=False)
        x = x + attn_out # residual_connection

        # Feed Forward Network (FFN)
        x = self.ln2(x)
        ffn_out = self.ffn(x)
        x = x + ffn_out # residual_connection
        
        return x
```