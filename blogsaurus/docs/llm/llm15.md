---
sidebar_position: 15
---

# [LLM] 15. í† í° ì…ë ¥ ë¶€í„° ë‹¨ì–´ ì˜ˆì¸¡ê¹Œì§€
---

## í…ìŠ¤íŠ¸ ìƒì„± ê³¼ì •
---

1. ì…ë ¥ ë°›ê¸°
- ì²˜ìŒì— ë¬¸ì¥ì„ ì…ë ¥ ë°›ìŠµë‹ˆë‹¤.

2. ë‹¤ìŒ ë‹¨ì–´ í™•ë¥  ê³„ì‚°
- ì…ë ¥ ë°›ì€ ë¬¸ì¥ ë‹¤ìŒì— ë‚˜ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

3. ë‹¨ì–´ ì„ íƒ (ë””ì½”ë”© ê¸°ë²•)
- ë¬´ì¡°ê±´ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.
- ì™œëƒí•˜ë©´ ë‹¤ì–‘í•œ í‘œí˜„ì„ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ !

### 1. Greedy Decoding

> ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ë¬´ì¡°ê±´ ì„ íƒí•˜ëŠ” ë°©ì‹

- Softmaxë¥¼ ì ìš©í•´ì„œ í™•ë¥  ê°€ì¥ ë†’ì€ ë‹¨ì–´ë¥¼ ë¬´ì¡°ê±´ ì„ íƒí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
- ì°½ì˜ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.


### 2. Temperature Scaling

> í™•ë¥  ì°¨ì´ë¥¼ ì¡°ì ˆí•´ì„œ ì°½ì˜ì„± ë†’ì´ëŠ” ë°©ì‹

- logitsë¥¼ temperatureë¡œ ë‚˜ëˆ„ì–´ì„œ í™œë¥  ë¶„í¬ë¥¼ ë³€í™”ì‹œí‚µë‹ˆë‹¤.
- 0.1~0.5 ê°’ì„ ë³´ìˆ˜ì ìœ¼ë¡œ ë³´ê³  1.0~2.0ì„ ì°½ì˜ì ìœ¼ë¡œ ë´…ë‹ˆë‹¤.


### 3. Top-k Sampling

> í™•ë¥ ì´ ë†’ì€ Kê°œì˜ ë‹¨ì–´ ì¤‘ ëœë¤í•˜ê²Œ ì„ íƒí•˜ëŠ” ë°©ì‹

- í™•ë¥ ì´ ë†’ì€ ìƒìœ„ Kê°œ ë‹¨ì–´ë§Œ í›„ë³´ë¡œ ë‚¨ê¸°ê³  ê·¸ ì¤‘ì—ì„œ ëœë¤ ì„ íƒí•©ë‹ˆë‹¤.
- Kê°’ì´ í¬ë©´ ë‹¤ì–‘ì„±ì´ ì¦ê°€í•˜ì§€ë§Œ í’ˆì§ˆì´ ë‚®ì•„ì§‘ë‹ˆë‹¤.


### 4. Nucleus Sampling (Top-p Sampling)

> í™•ë¥  í•©ì´ P(ì˜ˆë¥¼ ë“¤ì–´ 90%) ì´ìƒì´ ë˜ëŠ” ë‹¨ì–´ë“¤ë§Œ ì„ íƒ

- Top-KëŠ” ê°œìˆ˜ë¥¼ ì œí•œí•˜ì§€ë§Œ Nucleus Samplingì€ í™•ë¥ ì˜ ëˆ„ì í•© ê¸°ì¤€. (ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ)
- í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë“¤ì€ ìœ ë¦¬í•˜ì§€ë§Œ ë„ˆë¬´ ì´ìƒí•œ ë‹¨ì–´ëŠ” ìë™ìœ¼ë¡œ ê±¸ëŸ¬ì§„ë‹¤.


### 5. Beam Search

> ì—¬ëŸ¬ ê°œì˜ í›„ë³´ ë¬¸ì¥ì„ ë™ì‹œì— íƒìƒ‰í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì„ íƒ

- ì—¬ëŸ¬ ê°œì˜ í›„ë³´ ë¬¸ì¥ì„ ìœ ì§€í•˜ë©° íƒìƒ‰í•˜ê³  ê° ë¬¸ì¥ì˜ í™•ë¥  ê³„ì‚°í•´ì„œ ê°€ì¥ ë†’ì€ Nê°œë¥¼ ìœ ì§€ì‹œì¼œ ë†“ëŠ”ë‹¤.
- ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ê°€ì¥ ì¢‹ì€ ë¬¸ì¥ì„ ì„ íƒí•˜ê³  ì¶œë ¥í•œë‹¤.


## í…ìŠ¤íŠ¸ ìƒì„± ì‹¤ìŠµ
---

### Greedy Decoding

```python
# Greedy Decoding (íƒìš•ì  ì„ íƒ)
probs = F.softmax(logits, dim=-1)
greedy_idx = torch.argmax(probs).item()
plot_heatmap(probs, title="Greedy Decoding - Softmax Probabilities", selected_idx=greedy_idx)
```

![alt text](img/15/2.png)

`Greedy Decoding`ì€ ì‰½ê³  ì´í•´ë¥¼ ì‰½ê²Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëƒ¥ Softmax ì ìš© í›„ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

- âœ…ì¥ì 
    - ê³„ì‚° ì†ë„ ë¹ ë¦„
    - ë³µì¡í•œ í™•ë¥  ì¡°ì ˆ í•„ìš”í•˜ì§€ ì•ŠìŒ

- â›”ë‹¨ì 
    - ë§¤ë²ˆ ê°™ì€ ë‹¨ì–´ë§Œ ë‚˜ì˜´.
    - ê·¸ë˜ì„œ ë¬¸ì¥ ë‹¨ì¡°ë¡œì›Œì§.


### Temperature Scaling

```python
temperature = 0.8
scaled_logits = logits / temperature
probs_temp = F.softmax(scaled_logits, dim=-1)
temp_selected_idx = torch.multinomial(probs_temp, num_samples=1).item()
plot_heatmap(probs_temp, title=f"Temperature Scaling (T={temperature})", selected_idx=temp_selected_idx)
```

![alt text](img/15/3.png)

![alt text](img/15/4.png)

`Temperature Scaling`ì€ ì•„ë˜ì™€ ê°™ì€ ê³µì‹ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•©ë‹ˆë‹¤.

### $scaled\_logits = \frac{logits}{T}$ 

1. logitsì„ 1/Të°° ìŠ¤ì¼€ì¼ë§ í•˜ê³ 
2. Softmaxë¡œ í™•ë¥ í™” í•œ í›„
3. ë¶„í¬ì— ë”°ë¼ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì„ í•œë‹¤ê³  í•˜ë„¤ìš”.

ì´ê²Œ ë¬´ì‘ìœ„ê°€ ì§„ì§œ ë¬´ì‘ìœ„ëŠ” ì•„ë‹ˆê³  í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ë½‘ëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤.

ê·¸ë˜ì„œ Temperatureê°€ 1 ë¯¸ë§Œì´ë©´ ë†’ì€ ë¶€ë¶„ì— ì¹˜ìš°ì³ì„œ ìƒ˜í”Œë§í•˜ê³  
Temperatureê°€ 1ì´ë©´ ì›ë³¸ í™•ë¥ ëŒ€ë¡œ ë½‘ê³ 
Temperatureê°€ 1 ì´ˆê³¼í•˜ë©´ ë¶„í¬ê°€ í‰íƒ„í•´ì ¸ì„œ ëœë¤ì„±ì´ ì»¤ì§‘ë‹ˆë‹¤.
(ì™œ ëœë¤ì„±ì´ ì»¤ì§„ë‹¤ê³  í‘œí˜„í–ˆëƒë©´ í™•ë¥  ì°¨ì´ê°€ ì¤„ì–´ë“œë‹ˆê¹Œìš”)



### Top-K Sampling

Top-KëŠ” temperatureë¥¼ ì¢€ë” ë³´ì™„í•˜ëŠ” ë°©ì‹ì¸ë°. í™•ë¥ ì´ ë†’ì€ kê°œë¥¼ ê³¨ë¼ì„œ ê·¸ ì¤‘ì—ì„œ ëœë¤í•˜ê²Œ ê³ ë¥´ëŠ” ë°©ë²•ì´ë¼ê³  ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì—¬ê¸°ì„œ ëœë¤ë„ í™•ë¥ ì ìœ¼ë¡œ?)

ì´ëŸ° ë°©ì‹ì„ ì±„íƒí•˜ë©´ ë…¼ë¦¬ì ì´ë©´ì„œ ë‹¤ì–‘í•œ ë¬¸ì¥ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤ê³  í•˜ë„¤ìš”.

![alt text](img/15/5.png)



### Nucleus Sampling (Top-P)

ì–˜ëŠ” ì´ì œ ëˆ„ì í•©ì„ ì‚¬ìš©í•œ ì¹œêµ°ë° ëˆ„ì í•© P ì´ìƒë§Œ ëª¨ì•„ì„œ í™•ë¥ ì ìœ¼ë¡œ ê³ ë¥´ëŠ” ì•  ì…ë‹ˆë‹¤.

Top-Kë‘ ë¹„ìŠ·í•œë° Top-KëŠ” Kê°œë§Œ ê³ ë¥´ëŠ”ê±´ë° Top-PëŠ” ëˆ„ì í•© ì´ìƒë§Œ ë˜ë©´ OK ì…ë‹ˆë‹¤.

Top-Kë³´ë‹¤ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ. ì™œëƒí•˜ë©´ ì¢€ë” í›„ë³´ê°€ ë§ì•„ì§€ë‹ˆê¹Œ?

![alt text](img/15/6.png)


## í† í° ì„ë² ë”© ì½”ë“œ
---

```python

class GPTModel(nn.Module):
    def __init__(self, vocab_size,d_model,num_heads, num_layers,dropout,context_length):
        super().__init__()

        # 1. Token embedding (ë‹¨ì–´ > ë²¡í„°)
        self.token_embedding = nn.Embedding(vocab_size,d_model)

        # 2. position encoding
        self.position_encoding = nn.Embedding(context_length, d_model)
        
        # 3. transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, dropout, context_length) for _ in range(num_layers)
        ])

        # 4. Linear Projection
        self.ln_final = nn.LayerNorm(d_model)
        self.output_layer = nn.Linear(d_model,vocab_size)
        
    def forward(self, input_ids, mask=None):
        # 1. token embedding ì ìš©
        token_embeds = self.token_embedding(input_ids)

        # 2. position embedding ì ìš©
        pos_embeds = self.position_encoding(torch.arange(input_ids.shape[1],device=input_ids.device))

        # token embedding + position encoding
        x = token_embeds + pos_embeds

        # 4. transformer block í†µê³¼
        for block in self.blocks:
            x = block(x, mask)
        
        # 5. ìµœì¢…
        x = self.ln_final(x)
        logits = self.output_layer(x)

        return logits
```


### ì‹œê°í™” í•´ì„œ í™•ì¸

```python
# ëª¨ë¸ ì„¤ì •
vocab_size = 100    # ë‹¨ì–´ ê°œìˆ˜
d_model = 128       # ì„ë² ë”© ì°¨ì›
context_length = 20 # ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´

# GPT ëª¨ë¸ ìƒì„±
model = GPTModel(vocab_size,d_model,num_heads=4,num_layers=4,dropout=0.1,context_length=context_length)

# ê°€ì§œ ë°ì´í„°
input_ids = torch.randint(0,vocab_size,(1,context_length))

with torch.no_grad():
    token_embeds = model.token_embedding(input_ids).squeeze(0).numpy()  # (seq_length, d_model)
    pos_embeds = model.position_encoding(torch.arange(context_length)).numpy()  # (seq_length, d_model)
    combined_embeds = token_embeds + pos_embeds  # ìµœì¢… Transformer ì…ë ¥

# ğŸ”¹ íˆíŠ¸ë§µ ì‹œê°í™” (Embedding í™•ì¸)
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.heatmap(token_embeds, cmap="coolwarm", ax=axes[0])
axes[0].set_title("Token Embedding")

sns.heatmap(pos_embeds, cmap="coolwarm", ax=axes[1])
axes[1].set_title("Positional Encoding")

sns.heatmap(combined_embeds, cmap="coolwarm", ax=axes[2])
axes[2].set_title("Token + Positional Encoding")

plt.show()
```

![alt text](img/15/1.png)


## ì˜ˆì¸¡í•´ë³´ê¸°
---

```python
import torch
import torch.nn.functional as F

from embedding import GPTModel

# ì–´íœ˜ì§‘
vocabulary = ["hello","world","this","is","a","test","model","GPT","language","AI"]
vocabulary_size = len(vocabulary)

# í† í° ì¸ì½”ë”© í•¨ìˆ˜
def encode(text):
    return torch.tensor([vocabulary.index(word) for word in text.split() if word in vocabulary]).unsqueeze(0)

# í† í° ë””ì½”ë”© í•¨ìˆ˜
def decode(tokens):
    return " ".join([vocabulary[index] for index in tokens])

# ê°€ì§œ ì…ë ¥ ë¬¸ì¥
input_text = "hello world this is"
input_ids = encode(input_text)

# ëª¨ë¸ ìƒì„±
model = GPTModel(
    vocab_size=vocabulary_size, 
    d_model=128,
    num_heads=4,
    num_layers=4,
    dropout=0.1,
    context_length=vocabulary_size)

model.eval() # í‰ê°€ ëª¨ë“œ

with torch.no_grad():
    logits = model(input_ids)
    next_token_logits = logits[:,-1,:]
    next_token = torch.argmax(F.softmax(next_token_logits,dim=1),dim=-1).item()

# ê²°ê³¼ ì¶œë ¥
print(f"ì…ë ¥ ë¬¸ì¥: {input_text}")
print(f"ì˜ˆì¸¡ëœ ë‹¤ìŒ ë‹¨ì–´: {vocabulary[next_token]}")
```

ì•„ë˜ ì²˜ëŸ¼ ì¶œë ¥ë˜ëŠ”ë° ì–´ì°¨í”¼ ëœë¤ì…ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ëª¨ë¸ì„ í†µí•´ í•™ìŠµì´ ëœê²Œ ì•„ë‹ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

```text
ì…ë ¥ ë¬¸ì¥: hello world this is
ì˜ˆì¸¡ëœ ë‹¤ìŒ ë‹¨ì–´: test
```



### ì—¬ëŸ¬ê°œ ì˜ˆì¸¡í•˜ê¸°

```python
def generate(model,tokenizer,start_text,max_new_tokens):
    model.eval()
    input_ids=encode(start_text)

    for _  in range(max_new_tokens):
        with torch.no_grad():
            logits = model(input_ids)
            next_token_logits = logits[:,-1,:]
            next_token = torch.argmax(F.softmax(next_token_logits,dim=1),dim=-1).item()
            
            # ì…ë ¥ì— ì¶”ê°€
            input_ids = torch.cat([input_ids, torch.tensor([[next_token]])], dim=1)

    return decode(input_ids.squeeze(0).tolist())

# ì‹¤í–‰
generated_text = generate(model, encode, "hello world this is", max_new_tokens=5)
print("ğŸ”¥ ìƒì„±ëœ ë¬¸ì¥:", generated_text)
```



```text
ğŸ”¥ ìƒì„±ëœ ë¬¸ì¥: hello world this is a world model AI a
```

ì´ëŸ°ì‹ìœ¼ë¡œ ì—¬ëŸ¬ê°œ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.. í•™ìŠµì´ ëœ ëª¨ë¸ì´ ì•„ë‹ˆê¸°ì— ëœë¤ê°’ì…ë‹ˆë‹¤.




## ì „ì— ë‹¤ë¤˜ë˜ ì½”ë“œ
---

### Transformer Block
```python
class TransformerBlock(nn.Module):
    """
    d_model: ì…ë ¥ ì°¨ì› í¬ê¸°
    num_heads: Multi-Head Attentionì—ì„œ ì‚¬ìš©í•  Attention Head ê°œìˆ˜
    dropout: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    context_length: í•œë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´
    """
    def __init__(self, d_model, num_heads, dropout, context_length):
        super().__init__()

        self.context_length = context_length
        # Layer Normalization (Pre-LayerNorm ë°©ì‹)
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        # Multi-Head Attention (MHA)
        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)

        # Feed Forward Network (FFN)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),  # í™•ì¥ (ê¸°ë³¸ì ìœ¼ë¡œ 4ë°° í¬ê¸°ë¡œ ì¦ê°€)
            nn.GELU(),  # í™œì„±í™” í•¨ìˆ˜
            nn.Linear(d_model * 4, d_model),  # ë‹¤ì‹œ ì›ë˜ í¬ê¸°ë¡œ ì¶•ì†Œ
            nn.Dropout(dropout),
        )

        # ë¯¸ë¦¬ causal maskë¥¼ ë§Œë“¤ì–´ë‘ê¸°
        self.register_buffer("causal_mask", self._create_causal_mask(context_length))

    def _create_causal_mask(self, seq_length):
        """
        ìƒì‚¼ê° í–‰ë ¬ì„ ë§Œë“¤ì–´ì„œ ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ë„ë¡ í•¨.
        """
        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask

    def forward(self, x, mask=None):
        # ì…ë ¥ ì‹œí€€ìŠ¤ í™•ì¸
        seq_length = x.size(1)
        if mask is None:
            mask = self._create_causal_mask(seq_length).to(x.device) 

        # Multi-Head Attention (Self-Attention)
        x = self.ln1(x)
        attn_out,_ = self.attn(x,x,x,attn_mask=mask,need_weights=False)
        x = x + attn_out # residual_connection

        # Feed Forward Network (FFN)
        x = self.ln2(x)
        ffn_out = self.ffn(x)
        x = x + ffn_out # residual_connection
        
        return x
```