---
sidebar_position: 17
---

# [LLM] 17. ì‚¬ì „ í•™ìŠµ
---

ì‚¬ì „ í•™ìŠµì„ ì§„í–‰í•´ë´¤ìŠµë‹ˆë‹¤.

## ë°ì´í„° ì¤€ë¹„
---

```python
# í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ì¤€ë¹„
all_data = [
    ("What is the capital of France?", "The capital of France is Paris."),
    ("What is the capital of Germany?", "The capital of Germany is Berlin."),
    ("How are you?", "I am fine, thank you. And you?"),
    ("What is 2 + 2?", "2 + 2 equals 4."),
    ("Who wrote Harry Potter?", "J.K. Rowling wrote Harry Potter."),
    ("What is the tallest mountain?", "Mount Everest is the tallest mountain."),
    ("When was the first computer invented?", "The first electronic computer was invented in the 1940s."),
    ("What is the boiling point of water?", "Water boils at 100 degrees Celsius at sea level."),
    ("Who painted the Mona Lisa?", "Leonardo da Vinci painted the Mona Lisa."),
    ("What is the capital of Japan?", "The capital of Japan is Tokyo."),
]
```

ì´ëŸ° í›ˆë ¨ë°ì´í„°ë¡œ í•™ìŠµí•˜ë ¤ í–ˆìŠµë‹ˆë‹¤.

```python
def prepare_training_data(data):
    """ê° ì‘ë‹µ ëì— EOS í† í°ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€"""
    formatted_data = []
    for question, answer in data:
        # EOS í† í°ì´ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€ëœ ë‹µë³€
        formatted_answer = answer + tokenizer.eos_token
        formatted_data.append((question, formatted_answer))
    return formatted_data


tokenizer = GPT2Tokenizer.from_pretrained("gpt2")


# ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„í•  (80% í›ˆë ¨, 20% ê²€ì¦)
random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
random.shuffle(all_data)
train_size = int(0.8 * len(all_data))
training_data = all_data[:train_size]
validation_data = all_data[train_size:]

training_data = prepare_training_data(training_data)
validation_data = prepare_training_data(validation_data)
```

ì´ ë¬¸ì¥ë“¤ì„ í•™ìŠµí•  ë•Œ ê·¸ëƒ¥ ë’¤ì— EOSë¥¼ ë¶™ì—¬ì„œ í•™ìŠµí•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.


## ì¸ì½”ë”©
---

```python
def encode_text(full_text):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ê³  ì…ë ¥(input)ê³¼ íƒ€ê²Ÿ(target) ì‹œí€€ìŠ¤ë¡œ ë¶„ë¦¬
    ì…ë ¥: ì‹œí€€ìŠ¤ì˜ ì²˜ìŒë¶€í„° ë§ˆì§€ë§‰ í† í° ì´ì „ê¹Œì§€
    íƒ€ê²Ÿ: ì‹œí€€ìŠ¤ì˜ ë‘ ë²ˆì§¸ í† í°ë¶€í„° ëê¹Œì§€
    """
    encoded = tokenizer.encode(full_text, return_tensors="pt")
    input_ids = encoded[:, :-1]  # ë§ˆì§€ë§‰ í† í° ì œì™¸
    target_ids = encoded[:, 1:]  # ì²« ë²ˆì§¸ í† í° ì œì™¸
    return input_ids, target_ids

```

í•™ìŠµì„ ìœ„í•œ ì¸ì½”ë”© í•¨ìˆ˜ì…ë‹ˆë‹¤.

### ì¸ì½”ë”© í•¨ìˆ˜

`encode_text` í•¨ìˆ˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ì •ë‹µ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“œëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

"Hello World"ê°€ ë“¤ì–´ì™”ë‹¤ê³  ì˜ˆë¥¼ ë“¤ê³  ì•„ë˜ì²˜ëŸ¼ ê°’ì´ ìƒì„±ëë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤.


```python
encoded =tokenizer.encode("Hello world", return_tensors="pt")
# encoded = [[101, 7592, 2088, 102]]
```

- `101` -> ì‹œì‘ í† í°(CLS)
- `1234` -> "Hello"ì˜ í† í° ID
- `5678` -> "World"ì˜ í† í° ID
- `102` -> ì¢…ë£Œ í† í°(SEP)

ì…ë‹ˆë‹¤. ê·¸ëŸ¼ ì•„ë˜ì²˜ëŸ¼ ë˜ê² ì£ ?

```text
input_ids = [101,1234,5678]
target_ids = [1234,5678,102]
```

ì—¬ê¸°ê¹Œì§„ ì´í•´ê°€ ë©ë‹ˆë‹¤. ê·¸ëŸ¼. ê·¸ë˜ì„œ ë­? ë¼ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë­ëƒë©´

input_idsê°€ target_idsë¥¼ ë§ì¶°ì•¼í•˜ëŠ” ê²ë‹ˆë‹¤. í‘œë¡œ ë³´ë©´ ì´í•´ê°€ ë¹ ë¥¸ë°

|Step|Input|Target|
|---|-----|-----|
|1|101|1234|
|2|101, 1234|5678|
|3|101, 1234, 5678|102|

ì´ëŸ°ì‹ìœ¼ë¡œ Inputì´ ë“¤ì–´ì™”ì„ ë•Œ Targetì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµì‹œí‚¨ë‹¤ê³  í•©ë‹ˆë‹¤.

ì¦‰. Autoregressiveëª¨ë¸ì´ê¸° ë•Œë¬¸ì— í•œ ë‹¨ê³„ ì”© ì˜ˆì¸¡í•˜ë©´ì„œ í•™ìŠµ í•˜ëŠ” ê²ë‹ˆë‹¤.

`Hello`ê°€ ë“¤ì–´ì˜¤ë©´ `World`ê°€ ì˜¬ê±¸ ì˜ˆìƒí•˜ê³  `Hello World`ê°€ ì™„ì„±ë˜ë©´ ì¢…ë£Œí† í°ì„ ì˜ˆìƒí•˜ëŠ” ê²ë‹ˆë‹¤.


## í‰ê°€
---

```python
def evaluate(data_set):
    """ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ ê³„ì‚°"""
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    total_loss = 0
    
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        for input_text, target_text in data_set:
            # ì…ë ¥ê³¼ íƒ€ê²Ÿì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ì—°ê²°
            full_text = input_text + " " + target_text
            input_ids, target_ids = encode_text(full_text)
            input_ids, target_ids = input_ids.to(device), target_ids.to(device)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            logits = model(input_ids)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1))
            total_loss += loss.item()
    
    # í‰ê·  ì†ì‹¤ ë°˜í™˜
    return total_loss / len(data_set)
```

í›ˆë ¨ì— ëŒ€í•œ ì†ì‹¤ì„ í‰ê°€í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

no_gradë¡œ í•˜ëŠ” ì´ìœ ëŠ” ê¸°ìš¸ê¸°ê°€ í•„ìš”ì—†ì–´ì„œ ì…ë‹ˆë‹¤.

### íë¦„

```python
    full_text = input_text + " " + target_text
    input_ids, target_ids = encode_text(full_text)
    input_ids, target_ids = input_ids.to(device), target_ids.to(device)
```

ì§ˆë¬¸ê³¼ ì •ë‹µì„ ì¼ë‹¨ í•©ì³ë´…ë‹ˆë‹¤. (í•™ìŠµ í•  ìˆ˜ ìˆë„ë¡)
ê·¸ë¦¬ê³  tokenidë¡œ ë§Œë“­ë‹ˆë‹¤.


```python
    # ëª¨ë¸ ì˜ˆì¸¡
    logits = model(input_ids)
```

ì…ë ¥ í† í°ì•„ì´ë””ë¥¼ ëª¨ë¸ì— ë„£ì–´ì„œ ì˜ˆì¸¡ì„ ì§„í–‰í•©ë‹ˆë‹¤.

```python
    # ì†ì‹¤ ê³„ì‚°
    loss = criterion(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1))
    total_loss += loss.item()
```

`CrossEntropyLoss`ë¡œ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ê³„ì‚°í•œ ì†ì‹¤ì˜ totalì„ êµ¬í•´ë†“ê³  í‰ê· ì„ ë°˜í™˜í•©ë‹ˆë‹¤.


## í•™ìŠµ
---

```python
optimizer = optim.AdamW(model.parameters(), lr=5e-5)

# í•™ìŠµ ì‹œì‘
num_epochs = 10  # ì—í­ ìˆ˜ ì¦ê°€
best_val_loss = float('inf')
early_stopping_counter = 0
early_stopping_patience = 5  # 5ë²ˆ ì—°ì†ìœ¼ë¡œ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ

print("í•™ìŠµ ì‹œì‘...")
for epoch in range(num_epochs):
    model.train()  # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
    train_loss = 0
    
    # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    progress_bar = tqdm(training_data, desc=f"Epoch {epoch+1}/{num_epochs}")
    for input_text, target_text in progress_bar:
        # ì…ë ¥ê³¼ íƒ€ê²Ÿì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ì—°ê²°
        full_text = input_text + " " + target_text
        input_ids, target_ids = encode_text(full_text)
        input_ids, target_ids = input_ids.to(device), target_ids.to(device)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        optimizer.zero_grad()
        
        # ëª¨ë¸ ì˜ˆì¸¡
        logits = model(input_ids)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = criterion(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1))
        
        # ì—­ì „íŒŒ ë° ìµœì í™”
        loss.backward()
        optimizer.step()
        
        # ì†ì‹¤ ê¸°ë¡
        train_loss += loss.item()
        progress_bar.set_postfix({"train_loss": f"{loss.item():.4f}"})
    
    # í‰ê·  í›ˆë ¨ ì†ì‹¤ ê³„ì‚°
    avg_train_loss = train_loss / len(training_data)
    train_losses.append(avg_train_loss)
    
    # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ í‰ê°€
    val_loss = evaluate(validation_data)
    val_losses.append(val_loss)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"Epoch {epoch+1}/{num_epochs}, í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}, ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
    
    # ì¡°ê¸° ì¢…ë£Œ ê²€ì‚¬
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        early_stopping_counter = 0
        # ìµœìƒì˜ ëª¨ë¸ ì €ì¥
        torch.save(model.state_dict(), "best_model.pt")
        print(f"âœ… ê²€ì¦ ì†ì‹¤ ê°œì„ ! ëª¨ë¸ ì €ì¥ (ê²€ì¦ ì†ì‹¤: {val_loss:.4f})")
    else:
        early_stopping_counter += 1
        print(f"ê²€ì¦ ì†ì‹¤ ê°œì„  ì—†ìŒ ({early_stopping_counter}/{early_stopping_patience})")
        
    if early_stopping_counter >= early_stopping_patience:
        print(f"ì¡°ê¸° ì¢…ë£Œ: {early_stopping_patience}ë²ˆ ì—°ì†ìœ¼ë¡œ ê²€ì¦ ì†ì‹¤ ê°œì„  ì—†ìŒ")
        break
```

í•™ìŠµ ì‹œí‚¤ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

### íë¦„ ì„¤ëª…

- **optimizer**


```python
optimizer = optim.AdamW(model.parameters(), lr=5e-5)
```

optimizerë¥¼ ì„ ì–¸í•©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ì— ì‚¬ìš©ë©ë‹ˆë‹¤.


- **epoch**

```python
num_epochs = 10

for epoch in range(num_epochs):
    ...

```

epoch ìˆ˜ ë§Œí¼ í•™ìŠµì„ ë°˜ë³µí•©ë‹ˆë‹¤.

- **train mode**

```python
model.train()
```

modelì„ í•™ìŠµìš©ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.

- **tqdm**

```python
progress_bar = tqdm(training_data, desc=f"Epoch {epoch+1}/{num_epochs}")
```

`tqdm`ì€ `taquddum`ì˜ ì•½ìë¡œ ì§„í–‰ì´ë¼ëŠ” ëœ»ì„ ê°€ì§„ í•¨ìˆ˜ì¸ë° ì§„í–‰ì‚¬í•­ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤€ë‹¤.

- **get token id**

```python
    # ì…ë ¥ê³¼ íƒ€ê²Ÿì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ì—°ê²°
    full_text = input_text + " " + target_text
    input_ids, target_ids = encode_text(full_text)
    input_ids, target_ids = input_ids.to(device), target_ids.to(device)
```

full_textë¥¼ ë§Œë“¤ê³  token idë¥¼ ë°›ì•„ì˜¤ëŠ” ì½”ë“œì´ë‹¤.

- **zero grad**

```python
    # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
    optimizer.zero_grad()
```

ì´ì „ ê¸°ìš¸ê¸°ë¥¼ ì´ˆê¸°í™” ì‹œì¼œì£¼ëŠ” í•¨ìˆ˜ì¸ë° ì´ê±¸ í•´ì¤˜ì•¼ ë‹¤ìŒ í•™ìŠµ caseì— ì˜í–¥ì´ ê°€ì§€ ì•ŠìŠµë‹ˆë‹¤.

- **get loss**

```python
    # ëª¨ë¸ ì˜ˆì¸¡
    logits = model(input_ids)
    
    # ì†ì‹¤ ê³„ì‚°
    loss = criterion(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1))
```

ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°ì¸ë° ë„˜ì–´ê°ˆê²Œìš”


- **backward**

```python
    loss.backward()
```

ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

### í•™ìŠµ ì§„í–‰

í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê·¸ë ¤ì§€ë”ë¼êµ¬ìš”

![alt text](img/17/loss_graph.png)


```text
ğŸ’¬ ì…ë ¥: What is the capital of France?
ğŸ“ ì¶œë ¥: The capital of Japan is Tokyo.

ğŸ’¬ ì…ë ¥: How are you?
ğŸ“ ì¶œë ¥: I am fine, thank you. And you?

ğŸ’¬ ì…ë ¥: Who wrote Harry Potter?
ğŸ“ ì¶œë ¥: J.K. Rowling wrote Harry Potter.
```

ë‹µë„ ì˜ ì£¼ê³  ë°›ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.