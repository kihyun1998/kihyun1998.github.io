---
sidebar_position: 16
---

# [LLM] 16. í•™ìŠµ í‰ê°€
---


## GPTê°€ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ” ì›ë¦¬
---

GPT ëª¨ë¸ì€ ì£¼ì–´ì§„ ì…ë ¥ ë‹¤ìŒì— ì˜¬ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### ê³¼ì •

1. ì…ë ¥ê°’ ì„¤ì •
2. ëª¨ë¸ì´ ê° ë‹¨ê³„ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡í•˜ì—¬ í™•ë¥  ë¶„í¬ ìƒì„±
3. ì˜ˆì¸¡ëœ ë‹¨ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì¥ í™•ì¥
4. ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ê¸°ë²•ìœ¼ë¡œ ì°½ì˜ì  ê²°ê³¼ ì–»ê¸°


### ì˜ˆì œ ì½”ë“œ

```python
def generate_text(model, prompt, tokenizer, max_length=50, temperature=1.0, top_k=10):
    """
    GPTModelì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    - model: ì‚¬ì „ í›ˆë ¨ëœ GPT ëª¨ë¸
    - prompt: ì´ˆê¸° ì…ë ¥ ë¬¸ì¥ (str)
    - tokenizer: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    - max_length: ìƒì„±í•  ìµœëŒ€ í† í° ê¸¸ì´
    - temperature: ì°½ì˜ì„± ì¡°ì ˆ (ë†’ì„ìˆ˜ë¡ ëœë¤í•œ ê²°ê³¼)
    - top_k: ìƒìœ„ kê°œì˜ ë‹¨ì–´ë§Œ ìƒ˜í”Œë§í•˜ì—¬ ì„ íƒ

    return: ìƒì„±ëœ í…ìŠ¤íŠ¸ (str)
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # ì…ë ¥ í† í° ë³€í™˜
    input_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)

    # í…ìŠ¤íŠ¸ ìƒì„±
    for _ in range(max_length):
        with torch.no_grad():
            logits = model(input_ids)[:, -1, :]  # ë§ˆì§€ë§‰ ë‹¨ì–´ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ 
            logits = logits / temperature  # Temperature ì¡°ì •

            # Top-k ìƒ˜í”Œë§ ì ìš©
            top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)
            probs = F.softmax(top_k_logits, dim=-1)

            # í™•ë¥  ê¸°ë°˜ ìƒ˜í”Œë§
            sampled_token = torch.multinomial(probs, num_samples=1)

            # ìƒˆë¡œìš´ í† í° ì¶”ê°€
            input_ids = torch.cat([input_ids, top_k_indices.gather(-1, sampled_token)], dim=-1)

            # ì¢…ë£Œ ì¡°ê±´ (EOS í† í°)
            if input_ids[0, -1].item() == tokenizer.eos_token_id:
                break

    # ë””ì½”ë”©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶œë ¥
    return tokenizer.decode(input_ids[0].tolist())

```

tokenizerë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.


## GPT-2 tokenizer ì‚¬ìš© ë‹¨ì–´ ì˜ˆì¸¡
---

### ì½”ë“œ

```python
from text_generate import generate_text

from transformers import GPT2Tokenizer



import sys
sys.path.append("..")
from model.config import GPT_CONFIG_124M
from model.gpt_model import GPTModel


# ëª¨ë¸ ì´ˆê¸°í™”
model = GPTModel(GPT_CONFIG_124M)
model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •


# GPT-2 í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° (Hugging Face)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# ì˜ˆì œ í”„ë¡¬í”„íŠ¸
prompt = "Once upon a time"

# í…ìŠ¤íŠ¸ ìƒì„±
generated_text = generate_text(model, prompt, tokenizer, max_length=50, temperature=0.7, top_k=10)

# ê²°ê³¼ ì¶œë ¥
print("\nğŸ“œ Generated Text:\n", generated_text)
```


### ê²°ê³¼

```text
ğŸ“œ Generated Text:
 Once upon a timecars sketch invol ZIP balk locate McK mortality remain Week009 Hopefully compiledcoBer embargo everlasting eggsQuote situated breedforcer prooffamous paragraphs Indians inhumanitud FractLeadAuthorities remarks arminvokeCapture manifestation venom indic percentages centuries mortaliselpton expr encounteringAlthough crusCritics Prohibition Begins
```


## Huggin face tokenizer ì‚¬ìš© ë‹¨ì–´ ì˜ˆì¸¡
---

### ì½”ë“œ

```python
from transformers import AutoTokenizer
from text_generate import generate_text


import sys

sys.path.append("..")

from model.config import GPT_CONFIG_124M
from model.gpt_model import GPTModel

# í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/polyglot-ko-1.3b")

# ì›ë˜ PyTorch ëª¨ë¸ ì‚¬ìš©
model = GPTModel(GPT_CONFIG_124M)
model.eval()

# ì˜ˆì œ í”„ë¡¬í”„íŠ¸
prompt = "ì˜›ë‚  ì˜›ì ì—"

# í† í¬ë‚˜ì´ì €ë¡œ ì¸ì½”ë”©
inputs = tokenizer(prompt, return_tensors="pt", padding=True)

# ì—¬ê¸°ì„œ ì›ë˜ generate_text í•¨ìˆ˜ ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ ìƒì„± ë¡œì§ êµ¬í˜„
generated_text = generate_text(model, prompt, tokenizer, max_length=50, temperature=0.7, top_k=10)

print("\nğŸ“œ ìƒì„±ëœ í…ìŠ¤íŠ¸:\n", generated_text)
```

### ê²°ê³¼

```text
ğŸ“œ ìƒì„±ëœ í…ìŠ¤íŠ¸:
 ì˜›ë‚  ì˜›ì ì— a ì–´ë ´í’‹ ìŠ¤í…Œì´í¬ ë‚³ ìš©í’ˆ í‹±orm ì ˆê° ì–˜ê¸°ï¿½ ë³´ë‚´ë‹¤ì§€ë§Œ ë”°ë¼ì•¼ ì˜† ì—°ë§ì—°ì‹œ ë°©ì‚° ë²ˆê°œ í¬ë¥´ì‰ ì¤‘ê³„ì¸ë°ë‹¤ ë² íŠ¸ í•¨ê»˜ï¼…  ì‹¤ê±° ì‚¬ì‹¤ì¹˜ë§¤ì •ë³´ 72 ëŒ€ì¤‘êµí†µ ì°½ìë³¸ì£¼ì˜ë‹ë¼ì£¼ë§
```

ê°œíŒì…ë‹ˆë‹¤.