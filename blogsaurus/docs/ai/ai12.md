---
sidebar_position: 12
---

# 🧠 Whitepaper Companion Podcast 요약: Embeddings & Vector Stores
---

## 📑 목차
1. [소개 및 개요](#소개-및-개요)  
2. [Embedding이란?](#embedding이란)  
3. [Embedding의 핵심 응용 분야](#embedding의-핵심-응용-분야)  
4. [Embedding의 성능 평가 방법](#embedding의-성능-평가-방법)  
5. [RAG(Retrieval-Augmented Generation)](#ragretrieval-augmented-generation)  
6. [텍스트 Embedding: 워드/문서 임베딩](#텍스트-embedding-워드문서-임베딩)  
7. [이미지 및 멀티모달 Embedding](#이미지-및-멀티모달-embedding)  
8. [구조화 데이터 & 그래프 Embedding](#구조화-데이터--그래프-embedding)  
9. [Embedding 학습 방식](#embedding-학습-방식)  
10. [Vector Search & A&N 알고리즘](#vector-search--an-알고리즘)  
11. [Vector Database 시스템](#vector-database-시스템)  
12. [Embedding의 실제 응용 사례](#embedding의-실제-응용-사례)  
13. [운영 상 고려 사항 & 마무리](#운영-상-고려-사항--마무리)

---

## 소개 및 개요

- Embedding은 **텍스트, 이미지, 오디오 등 모든 데이터를 벡터로 표현**하는 방법.
- **의미(semantic)를 숫자 공간에 매핑**하여 유사도, 검색, 추천에 활용.
- 특히 Kaggle과 같은 실무 환경에서 데이터 처리 효율과 통찰력 향상에 핵심적.

---

## Embedding이란?

- **고차원 데이터를 저차원 공간에 의미를 유지한 채로 표현**.
- 예: "king"과 "queen"은 유사한 의미 → 임베딩 공간에서 가까움.
- 아날로지: 위도/경도로 지구상의 위치를 표시하는 것처럼, 복잡한 개념을 좌표로 표현.

---

## Embedding의 핵심 응용 분야

1. **정보 검색 (Retrieval)**  
   - 검색 쿼리와 문서 모두 임베딩 → 가장 가까운 벡터 찾기.
   - 예: Google 검색은 웹페이지 임베딩 → 쿼리 임베딩과 비교.

2. **추천 시스템**  
   - 사용자 및 아이템을 동일 임베딩 공간에 맵핑 → 유사도 기반 추천.

3. **Joint Embedding (멀티모달)**  
   - 서로 다른 타입(텍스트, 이미지 등)을 **하나의 공간에 임베딩**.
   - 예: 텍스트로 이미지 검색하기.

---

## Embedding의 성능 평가 방법

- **Precision / Recall**: 얼마나 정확하고 충분한 결과를 가져오는가?
- **Precision@K / Recall@K**: 상위 K개 결과 기준 평가.
- **NDCG (Normalized Discounted Cumulative Gain)**: **정답 순서**도 반영하는 랭킹 품질 지표.
- **Benchmark 데이터셋**: BEIR, MTEB 등으로 모델 간 성능 비교 가능.
- **평가 도구**: `Tevatron`, `Treccast`, `Pythia` 등 추천됨.

---

## RAG(Retrieval-Augmented Generation)

- 검색 + 생성형 AI를 결합한 구조.
- **2단계 과정**:
  1. 문서 → chunk → embedding → vector store에 저장 (인덱스 생성)
  2. 쿼리 임베딩 생성 → 가장 유사한 chunk 검색 → LLM에 추가해 응답 생성
- **빠른 벡터 검색이 핵심 요소**.

---

## 텍스트 Embedding: 워드/문서 임베딩

### ▶ Word Embeddings

- 초기 방식: **Word2Vec, GloVe, Swivel, FastText**
- Word2Vec: 주변 단어로 중심 단어 예측 (CBOW / Skip-gram)
- GloVe: 전체 코퍼스의 단어 공기 행렬(co-occurrence matrix) 기반
- 시각화 예제: Gensim + T-SNE 사용

### ▶ Document Embeddings

- 초기: **TF-IDF, BM25, LSA, LDA**
- Doc2Vec: 문단 벡터 추가 → 문서 표현
- 최신 방식: **BERT, Sentence-BERT, SimCSE, T5, Gemini**
- **문맥 기반 임베딩** → 같은 단어라도 다른 문맥이면 다른 벡터 생성
- Vertex AI, TensorFlow Hub로 쉽게 사용 가능

---

## 이미지 및 멀티모달 Embedding

- **이미지**: CNN 또는 Vision Transformer에서 추출한 피처를 사용
- **멀티모달**: 텍스트 + 이미지 결합 (예: KPaLI, CoLBERT-XR)
- 예시: Vertex AI API를 이용한 이미지 임베딩 및 검색

---

## 구조화 데이터 & 그래프 Embedding

### ▶ Structured Data

- 표 형식 데이터 → 차원 축소(PCA 등)로 행(row) 임베딩 생성
- 사용자-아이템 임베딩 → 추천 시스템

### ▶ Graph Embedding

- 예: 소셜 네트워크의 사람 및 연결 관계를 벡터로 표현
- 알고리즘: **DeepWalk, Node2Vec, LINE, GraphSAGE**

---

## Embedding 학습 방식

- **Dual Encoder 구조**: 쿼리/문서 각각 인코더로 임베딩
- **Contrastive Loss**: 유사한 쌍은 가깝게, 다른 쌍은 멀게 학습
- **Pretraining → Fine-tuning** 방식
  - Pretrain: 대규모 코퍼스, Foundation model 활용
  - Fine-tune: Task-specific 데이터셋
- 데이터 생성 방식: 수작업, LLM 기반 합성, Distillation, Hard negative mining

---

## Vector Search & A&N 알고리즘

- **Vector Search**: 의미 기반 검색. 쿼리 임베딩과 가장 가까운 결과 반환
- **A&N (Approximate Nearest Neighbor)**: 빠른 근사 유사도 탐색

| 기법 | 설명 |
|------|------|
| LSH | 유사 벡터를 같은 bucket에 할당 (예: 우편번호) |
| KD Tree / Ball Tree | 공간 분할 기반 탐색 |
| HNSW | 계층적 그래프 기반 탐색. 빠르고 정확함 |
| SCaNN | Google의 대규모용 탐색 알고리즘 |

---

## Vector Database 시스템

- 벡터를 효율적으로 저장하고 검색하는 전용 DB
- **하이브리드 검색**도 가능 (벡터 + 키워드)
- 주요 솔루션:
  - **Vertex AI Vector Search**
  - **Pinecone**, **Weaviate**, **Chromadb**
  - **PostgreSQL + pgvector**, **Milvus**

---

## Embedding의 실제 응용 사례

- 정보 검색
- 추천 시스템
- 유사 텍스트 판단
- 클러스터링, 분류
- RAG
- 이상 탐지
- 대규모 랭킹 시스템
- Few-shot 분류기

---

## 운영 상 고려 사항 & 마무리

- **임베딩 모델이 업데이트되면 기존 벡터도 재계산 필요**
- **보안, 백업, 스케일링** 모두 고려해야 함
- **단순 키워드 검색만으로는 부족할 때**, Embedding이 큰 차이를 만든다
- Kaggle 사용자라면, Embedding + Vector Store는 **반드시 익혀야 할 스킬**