"use strict";(self.webpackChunkblogsaurus=self.webpackChunkblogsaurus||[]).push([[26715],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>p});var o=t(67294);function l(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){l(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,o,l=function(e,n){if(null==e)return{};var t,o,l={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(l[t]=e[t]);return l}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}var s=o.createContext({}),d=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},m=function(e){var n=d(e.components);return o.createElement(s.Provider,{value:n},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},_=o.forwardRef((function(e,n){var t=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),u=d(t),_=l,p=u["".concat(s,".").concat(_)]||u[_]||c[_]||r;return t?o.createElement(p,a(a({ref:n},m),{},{components:t})):o.createElement(p,a({ref:n},m))}));function p(e,n){var t=arguments,l=n&&n.mdxType;if("string"==typeof e||l){var r=t.length,a=new Array(r);a[0]=_;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[u]="string"==typeof e?e:l,a[1]=i;for(var d=2;d<r;d++)a[d]=t[d];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}_.displayName="MDXCreateElement"},49569:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>c,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var o=t(87462),l=(t(67294),t(3905));const r={sidebar_position:15},a="[LLM] 15. \ud1a0\ud070 \uc785\ub825 \ubd80\ud130 \ub2e8\uc5b4 \uc608\uce21\uae4c\uc9c0",i={unversionedId:"llm/llm15",id:"llm/llm15",title:"[LLM] 15. \ud1a0\ud070 \uc785\ub825 \ubd80\ud130 \ub2e8\uc5b4 \uc608\uce21\uae4c\uc9c0",description:"---",source:"@site/docs/llm/llm15.md",sourceDirName:"llm",slug:"/llm/llm15",permalink:"/docs/llm/llm15",draft:!1,tags:[],version:"current",sidebarPosition:15,frontMatter:{sidebar_position:15},sidebar:"aiSidebar",previous:{title:"[LLM] 14. Transformer Block \uc5f0\uacb0\ud558\uae30",permalink:"/docs/llm/llm14"}},s={},d=[{value:"\ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uacfc\uc815",id:"\ud14d\uc2a4\ud2b8-\uc0dd\uc131-\uacfc\uc815",level:2},{value:"1. Greedy Decoding",id:"1-greedy-decoding",level:3},{value:"2. Temperature Scaling",id:"2-temperature-scaling",level:3},{value:"\ud1a0\ud070 \uc784\ubca0\ub529 \ucf54\ub4dc",id:"\ud1a0\ud070-\uc784\ubca0\ub529-\ucf54\ub4dc",level:2},{value:"\uc2dc\uac01\ud654 \ud574\uc11c \ud655\uc778",id:"\uc2dc\uac01\ud654-\ud574\uc11c-\ud655\uc778",level:3},{value:"\uc608\uce21\ud574\ubcf4\uae30",id:"\uc608\uce21\ud574\ubcf4\uae30",level:2},{value:"\uc5ec\ub7ec\uac1c \uc608\uce21\ud558\uae30",id:"\uc5ec\ub7ec\uac1c-\uc608\uce21\ud558\uae30",level:3},{value:"\uc804\uc5d0 \ub2e4\ub918\ub358 \ucf54\ub4dc",id:"\uc804\uc5d0-\ub2e4\ub918\ub358-\ucf54\ub4dc",level:2},{value:"Transformer Block",id:"transformer-block",level:3}],m={toc:d},u="wrapper";function c(e){let{components:n,...r}=e;return(0,l.kt)(u,(0,o.Z)({},m,r,{components:n,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"llm-15-\ud1a0\ud070-\uc785\ub825-\ubd80\ud130-\ub2e8\uc5b4-\uc608\uce21\uae4c\uc9c0"},"[LLM]"," 15. \ud1a0\ud070 \uc785\ub825 \ubd80\ud130 \ub2e8\uc5b4 \uc608\uce21\uae4c\uc9c0"),(0,l.kt)("hr",null),(0,l.kt)("h2",{id:"\ud14d\uc2a4\ud2b8-\uc0dd\uc131-\uacfc\uc815"},"\ud14d\uc2a4\ud2b8 \uc0dd\uc131 \uacfc\uc815"),(0,l.kt)("hr",null),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"\uc785\ub825 \ubc1b\uae30")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"\ucc98\uc74c\uc5d0 \ubb38\uc7a5\uc744 \uc785\ub825 \ubc1b\uc2b5\ub2c8\ub2e4.")),(0,l.kt)("ol",{start:2},(0,l.kt)("li",{parentName:"ol"},"\ub2e4\uc74c \ub2e8\uc5b4 \ud655\ub960 \uacc4\uc0b0")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"\uc785\ub825 \ubc1b\uc740 \ubb38\uc7a5 \ub2e4\uc74c\uc5d0 \ub098\uc62c \ub2e8\uc5b4\ub97c \uc608\uce21\ud569\ub2c8\ub2e4.")),(0,l.kt)("ol",{start:3},(0,l.kt)("li",{parentName:"ol"},"\ub2e8\uc5b4 \uc120\ud0dd (\ub514\ucf54\ub529 \uae30\ubc95)")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"\ubb34\uc870\uac74 \uac00\uc7a5 \ub192\uc740 \ud655\ub960\uc758 \ub2e8\uc5b4\ub97c \uc120\ud0dd\ud558\uc9c0\ub294 \uc54a\ub294\ub2e4."),(0,l.kt)("li",{parentName:"ul"},"\uc65c\ub0d0\ud558\uba74 \ub2e4\uc591\ud55c \ud45c\ud604\uc744 \ud560 \uc218 \uc788\uae30 \ub54c\ubb38 !")),(0,l.kt)("h3",{id:"1-greedy-decoding"},"1. Greedy Decoding"),(0,l.kt)("p",null,"\uac00\uc7a5 \ub192\uc740 \ud655\ub960\uc744 \ubb34\uc870\uac74 \uc120\ud0dd\ud558\ub294 \ubc29\uc2dd"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Softmax\ub97c \uc801\uc6a9\ud574\uc11c \ud655\ub960 \uac00\uc7a5 \ub192\uc740 \ub2e8\uc5b4\ub97c \ubb34\uc870\uac74 \uc120\ud0dd\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4."),(0,l.kt)("li",{parentName:"ul"},"\ucc3d\uc758\uc131\uc774 \ubd80\uc871\ud569\ub2c8\ub2e4.")),(0,l.kt)("h3",{id:"2-temperature-scaling"},"2. Temperature Scaling"),(0,l.kt)("p",null,"\ud655\ub960 \ucc28\uc774\ub97c \uc870\uc808\ud574\uc11c \ucc3d\uc758\uc131 \ub192\uc774\ub294 \ubc29\uc2dd"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"logits\ub97c temperature\ub85c \ub098\ub204\uc5b4\uc11c \ud65c\ub960 \ubd84\ud3ec\ub97c \ubcc0\ud654\uc2dc\ud0b5\ub2c8\ub2e4."),(0,l.kt)("li",{parentName:"ul"},"0.1~0.5 \uac12\uc744 \ubcf4\uc218\uc801\uc73c\ub85c \ubcf4\uace0 1.0~2.0\uc744 \ucc3d\uc758\uc801\uc73c\ub85c \ubd05\ub2c8\ub2e4.")),(0,l.kt)("h2",{id:"\ud1a0\ud070-\uc784\ubca0\ub529-\ucf54\ub4dc"},"\ud1a0\ud070 \uc784\ubca0\ub529 \ucf54\ub4dc"),(0,l.kt)("hr",null),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"\nclass GPTModel(nn.Module):\n    def __init__(self, vocab_size,d_model,num_heads, num_layers,dropout,context_length):\n        super().__init__()\n\n        # 1. Token embedding (\ub2e8\uc5b4 > \ubca1\ud130)\n        self.token_embedding = nn.Embedding(vocab_size,d_model)\n\n        # 2. position encoding\n        self.position_encoding = nn.Embedding(context_length, d_model)\n        \n        # 3. transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, num_heads, dropout, context_length) for _ in range(num_layers)\n        ])\n\n        # 4. Linear Projection\n        self.ln_final = nn.LayerNorm(d_model)\n        self.output_layer = nn.Linear(d_model,vocab_size)\n        \n    def forward(self, input_ids, mask=None):\n        # 1. token embedding \uc801\uc6a9\n        token_embeds = self.token_embedding(input_ids)\n\n        # 2. position embedding \uc801\uc6a9\n        pos_embeds = self.position_encoding(torch.arange(input_ids.shape[1],device=input_ids.device))\n\n        # token embedding + position encoding\n        x = token_embeds + pos_embeds\n\n        # 4. transformer block \ud1b5\uacfc\n        for block in self.blocks:\n            x = block(x, mask)\n        \n        # 5. \ucd5c\uc885\n        x = self.ln_final(x)\n        logits = self.output_layer(x)\n\n        return logits\n")),(0,l.kt)("h3",{id:"\uc2dc\uac01\ud654-\ud574\uc11c-\ud655\uc778"},"\uc2dc\uac01\ud654 \ud574\uc11c \ud655\uc778"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# \ubaa8\ub378 \uc124\uc815\nvocab_size = 100    # \ub2e8\uc5b4 \uac1c\uc218\nd_model = 128       # \uc784\ubca0\ub529 \ucc28\uc6d0\ncontext_length = 20 # \ucd5c\ub300 \ubb38\uc7a5 \uae38\uc774\n\n# GPT \ubaa8\ub378 \uc0dd\uc131\nmodel = GPTModel(vocab_size,d_model,num_heads=4,num_layers=4,dropout=0.1,context_length=context_length)\n\n# \uac00\uc9dc \ub370\uc774\ud130\ninput_ids = torch.randint(0,vocab_size,(1,context_length))\n\nwith torch.no_grad():\n    token_embeds = model.token_embedding(input_ids).squeeze(0).numpy()  # (seq_length, d_model)\n    pos_embeds = model.position_encoding(torch.arange(context_length)).numpy()  # (seq_length, d_model)\n    combined_embeds = token_embeds + pos_embeds  # \ucd5c\uc885 Transformer \uc785\ub825\n\n# \ud83d\udd39 \ud788\ud2b8\ub9f5 \uc2dc\uac01\ud654 (Embedding \ud655\uc778)\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nsns.heatmap(token_embeds, cmap="coolwarm", ax=axes[0])\naxes[0].set_title("Token Embedding")\n\nsns.heatmap(pos_embeds, cmap="coolwarm", ax=axes[1])\naxes[1].set_title("Positional Encoding")\n\nsns.heatmap(combined_embeds, cmap="coolwarm", ax=axes[2])\naxes[2].set_title("Token + Positional Encoding")\n\nplt.show()\n')),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"alt text",src:t(5912).Z,width:"1800",height:"500"})),(0,l.kt)("h2",{id:"\uc608\uce21\ud574\ubcf4\uae30"},"\uc608\uce21\ud574\ubcf4\uae30"),(0,l.kt)("hr",null),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'import torch\nimport torch.nn.functional as F\n\nfrom embedding import GPTModel\n\n# \uc5b4\ud718\uc9d1\nvocabulary = ["hello","world","this","is","a","test","model","GPT","language","AI"]\nvocabulary_size = len(vocabulary)\n\n# \ud1a0\ud070 \uc778\ucf54\ub529 \ud568\uc218\ndef encode(text):\n    return torch.tensor([vocabulary.index(word) for word in text.split() if word in vocabulary]).unsqueeze(0)\n\n# \ud1a0\ud070 \ub514\ucf54\ub529 \ud568\uc218\ndef decode(tokens):\n    return " ".join([vocabulary[index] for index in tokens])\n\n# \uac00\uc9dc \uc785\ub825 \ubb38\uc7a5\ninput_text = "hello world this is"\ninput_ids = encode(input_text)\n\n# \ubaa8\ub378 \uc0dd\uc131\nmodel = GPTModel(\n    vocab_size=vocabulary_size, \n    d_model=128,\n    num_heads=4,\n    num_layers=4,\n    dropout=0.1,\n    context_length=vocabulary_size)\n\nmodel.eval() # \ud3c9\uac00 \ubaa8\ub4dc\n\nwith torch.no_grad():\n    logits = model(input_ids)\n    next_token_logits = logits[:,-1,:]\n    next_token = torch.argmax(F.softmax(next_token_logits,dim=1),dim=-1).item()\n\n# \uacb0\uacfc \ucd9c\ub825\nprint(f"\uc785\ub825 \ubb38\uc7a5: {input_text}")\nprint(f"\uc608\uce21\ub41c \ub2e4\uc74c \ub2e8\uc5b4: {vocabulary[next_token]}")\n')),(0,l.kt)("p",null,"\uc544\ub798 \ucc98\ub7fc \ucd9c\ub825\ub418\ub294\ub370 \uc5b4\ucc28\ud53c \ub79c\ub364\uc785\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \ubaa8\ub378\uc744 \ud1b5\ud574 \ud559\uc2b5\uc774 \ub41c\uac8c \uc544\ub2c8\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-text"},"\uc785\ub825 \ubb38\uc7a5: hello world this is\n\uc608\uce21\ub41c \ub2e4\uc74c \ub2e8\uc5b4: test\n")),(0,l.kt)("h3",{id:"\uc5ec\ub7ec\uac1c-\uc608\uce21\ud558\uae30"},"\uc5ec\ub7ec\uac1c \uc608\uce21\ud558\uae30"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def generate(model,tokenizer,start_text,max_new_tokens):\n    model.eval()\n    input_ids=encode(start_text)\n\n    for _  in range(max_new_tokens):\n        with torch.no_grad():\n            logits = model(input_ids)\n            next_token_logits = logits[:,-1,:]\n            next_token = torch.argmax(F.softmax(next_token_logits,dim=1),dim=-1).item()\n            \n            # \uc785\ub825\uc5d0 \ucd94\uac00\n            input_ids = torch.cat([input_ids, torch.tensor([[next_token]])], dim=1)\n\n    return decode(input_ids.squeeze(0).tolist())\n\n# \uc2e4\ud589\ngenerated_text = generate(model, encode, "hello world this is", max_new_tokens=5)\nprint("\ud83d\udd25 \uc0dd\uc131\ub41c \ubb38\uc7a5:", generated_text)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-text"},"\ud83d\udd25 \uc0dd\uc131\ub41c \ubb38\uc7a5: hello world this is a world model AI a\n")),(0,l.kt)("p",null,"\uc774\ub7f0\uc2dd\uc73c\ub85c \uc5ec\ub7ec\uac1c \uc608\uce21\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.. \ud559\uc2b5\uc774 \ub41c \ubaa8\ub378\uc774 \uc544\ub2c8\uae30\uc5d0 \ub79c\ub364\uac12\uc785\ub2c8\ub2e4."),(0,l.kt)("h2",{id:"\uc804\uc5d0-\ub2e4\ub918\ub358-\ucf54\ub4dc"},"\uc804\uc5d0 \ub2e4\ub918\ub358 \ucf54\ub4dc"),(0,l.kt)("hr",null),(0,l.kt)("h3",{id:"transformer-block"},"Transformer Block"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'class TransformerBlock(nn.Module):\n    """\n    d_model: \uc785\ub825 \ucc28\uc6d0 \ud06c\uae30\n    num_heads: Multi-Head Attention\uc5d0\uc11c \uc0ac\uc6a9\ud560 Attention Head \uac1c\uc218\n    dropout: \uacfc\uc801\ud569 \ubc29\uc9c0\ub97c \uc704\ud55c \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728\n    context_length: \ud55c\ubc88\uc5d0 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ubb38\uc7a5 \uae38\uc774\n    """\n    def __init__(self, d_model, num_heads, dropout, context_length):\n        super().__init__()\n\n        self.context_length = context_length\n        # Layer Normalization (Pre-LayerNorm \ubc29\uc2dd)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n        \n        # Multi-Head Attention (MHA)\n        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n\n        # Feed Forward Network (FFN)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),  # \ud655\uc7a5 (\uae30\ubcf8\uc801\uc73c\ub85c 4\ubc30 \ud06c\uae30\ub85c \uc99d\uac00)\n            nn.GELU(),  # \ud65c\uc131\ud654 \ud568\uc218\n            nn.Linear(d_model * 4, d_model),  # \ub2e4\uc2dc \uc6d0\ub798 \ud06c\uae30\ub85c \ucd95\uc18c\n            nn.Dropout(dropout),\n        )\n\n        # \ubbf8\ub9ac causal mask\ub97c \ub9cc\ub4e4\uc5b4\ub450\uae30\n        self.register_buffer("causal_mask", self._create_causal_mask(context_length))\n\n    def _create_causal_mask(self, seq_length):\n        """\n        \uc0c1\uc0bc\uac01 \ud589\ub82c\uc744 \ub9cc\ub4e4\uc5b4\uc11c \ubbf8\ub798 \ub2e8\uc5b4\ub97c \ubcf4\uc9c0 \ubabb\ud558\ub3c4\ub85d \ud568.\n        """\n        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float(\'-inf\'))\n        return mask\n\n    def forward(self, x, mask=None):\n        # \uc785\ub825 \uc2dc\ud000\uc2a4 \ud655\uc778\n        seq_length = x.size(1)\n        if mask is None:\n            mask = self._create_causal_mask(seq_length).to(x.device) \n\n        # Multi-Head Attention (Self-Attention)\n        x = self.ln1(x)\n        attn_out,_ = self.attn(x,x,x,attn_mask=mask,need_weights=False)\n        x = x + attn_out # residual_connection\n\n        # Feed Forward Network (FFN)\n        x = self.ln2(x)\n        ffn_out = self.ffn(x)\n        x = x + ffn_out # residual_connection\n        \n        return x\n')))}c.isMDXComponent=!0},5912:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/1-6fff041c825efa18ff6a1603b3fc6d44.png"}}]);