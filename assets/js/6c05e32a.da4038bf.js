"use strict";(self.webpackChunkblogsaurus=self.webpackChunkblogsaurus||[]).push([[10734],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>f});var l=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);n&&(l=l.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,l)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function r(e,n){if(null==e)return{};var t,l,a=function(e,n){if(null==e)return{};var t,l,a={},i=Object.keys(e);for(l=0;l<i.length;l++)t=i[l],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(l=0;l<i.length;l++)t=i[l],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=l.createContext({}),d=function(e){var n=l.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},m=function(e){var n=d(e.components);return l.createElement(s.Provider,{value:n},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return l.createElement(l.Fragment,{},n)}},c=l.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,m=r(e,["components","mdxType","originalType","parentName"]),u=d(t),c=a,f=u["".concat(s,".").concat(c)]||u[c]||p[c]||i;return t?l.createElement(f,o(o({ref:n},m),{},{components:t})):l.createElement(f,o({ref:n},m))}));function f(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=c;var r={};for(var s in n)hasOwnProperty.call(n,s)&&(r[s]=n[s]);r.originalType=e,r[u]="string"==typeof e?e:a,o[1]=r;for(var d=2;d<i;d++)o[d]=t[d];return l.createElement.apply(null,o)}return l.createElement.apply(null,t)}c.displayName="MDXCreateElement"},76254:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>d});var l=t(87462),a=(t(67294),t(3905));const i={sidebar_position:13},o="[LLM] 13. Transformer Block \uc2e4\uc2b5",r={unversionedId:"llm/llm13",id:"llm/llm13",title:"[LLM] 13. Transformer Block \uc2e4\uc2b5",description:"---",source:"@site/docs/llm/llm13.md",sourceDirName:"llm",slug:"/llm/llm13",permalink:"/docs/llm/llm13",draft:!1,tags:[],version:"current",sidebarPosition:13,frontMatter:{sidebar_position:13},sidebar:"aiSidebar",previous:{title:"[LLM] 12. Transformer Block \uc774\ud574\ud558\uae30",permalink:"/docs/llm/llm12"}},s={},d=[{value:"\uacc4\ud68d",id:"\uacc4\ud68d",level:2},{value:"\uc785\ub825 (Token Embedding + Positional Encoding)",id:"\uc785\ub825-token-embedding--positional-encoding",level:2},{value:"\uacb0\uacfc \uc774\ubbf8\uc9c0",id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0",level:3},{value:"LayerNorm (Self-Attention \uc804\uc5d0 \uc815\uaddc\ud654)",id:"layernorm-self-attention-\uc804\uc5d0-\uc815\uaddc\ud654",level:2},{value:"\uacb0\uacfc \uc774\ubbf8\uc9c0",id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-1",level:3},{value:"Multi-Head Self-Attention (\ubb38\uc7a5\uc5d0\uc11c \uc911\uc694\ud55c \ub2e8\uc5b4 \ucc3e\uae30)",id:"multi-head-self-attention-\ubb38\uc7a5\uc5d0\uc11c-\uc911\uc694\ud55c-\ub2e8\uc5b4-\ucc3e\uae30",level:2},{value:"\uacb0\uacfc \uc774\ubbf8\uc9c0",id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-2",level:3},{value:"Residual Connection (\uc6d0\ub798 \uc815\ubcf4 \uc720\uc9c0)",id:"residual-connection-\uc6d0\ub798-\uc815\ubcf4-\uc720\uc9c0",level:2},{value:"\uacb0\uacfc \uc774\ubbf8\uc9c0",id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-3",level:3},{value:"LayerNorm (FFN \uc804\uc5d0 \uc815\uaddc\ud654)",id:"layernorm-ffn-\uc804\uc5d0-\uc815\uaddc\ud654",level:2},{value:"\uacb0\uacfc \uc774\ubbf8\uc9c0",id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-4",level:3},{value:"Feed Forward Network (FFN, Self-Attention \uacb0\uacfc\ub97c \uc815\uc81c)",id:"feed-forward-network-ffn-self-attention-\uacb0\uacfc\ub97c-\uc815\uc81c",level:2},{value:"\uacb0\uacfc \uc774\ubbf8\uc9c0",id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-5",level:3},{value:"Residual Connection (\uc6d0\ub798 \uc815\ubcf4 \uc720\uc9c0)",id:"residual-connection-\uc6d0\ub798-\uc815\ubcf4-\uc720\uc9c0-1",level:2},{value:"\uacb0\uacfc \uc774\ubbf8\uc9c0",id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-6",level:3}],m={toc:d},u="wrapper";function p(e){let{components:n,...i}=e;return(0,a.kt)(u,(0,l.Z)({},m,i,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"llm-13-transformer-block-\uc2e4\uc2b5"},"[LLM]"," 13. Transformer Block \uc2e4\uc2b5"),(0,a.kt)("hr",null),(0,a.kt)("p",null,"\uac04\ub2e8\ud55c Transformer Block\uc758 \ud750\ub984\uc744 \ub530\ub77c \ud788\ud2b8\ub9f5\uc744 \ubcf4\uba70 \uc5b4\ub5bb\uac8c \ubcc0\ud654\ud558\ub294\uc9c0 \uad00\ucc30\ud574\ubcf4\ub824\ud569\ub2c8\ub2e4."),(0,a.kt)("h2",{id:"\uacc4\ud68d"},"\uacc4\ud68d"),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-text"},"\uc785\ub825 (Token Embedding + Positional Encoding)  \n    \u2193  \nLayerNorm (Self-Attention \uc804\uc5d0 \uc815\uaddc\ud654)  \n    \u2193  \nMulti-Head Self-Attention (\ubb38\uc7a5\uc5d0\uc11c \uc911\uc694\ud55c \ub2e8\uc5b4 \ucc3e\uae30)  \n    \u2193  \nResidual Connection (\uc6d0\ub798 \uc815\ubcf4 \uc720\uc9c0)  \n    \u2193  \nLayerNorm (FFN \uc804\uc5d0 \uc815\uaddc\ud654)  \n    \u2193  \nFeed Forward Network (FFN, Self-Attention \uacb0\uacfc\ub97c \uc815\uc81c)  \n    \u2193  \nResidual Connection (\uc6d0\ub798 \uc815\ubcf4 \uc720\uc9c0)  \n    \u2193  \n\ucd9c\ub825 (\ub2e4\uc74c Transformer \ube14\ub85d\uc73c\ub85c \uc804\ub2ec or \ucd5c\uc885 \uc608\uce21)\n\n")),(0,a.kt)("p",null,"\uc774 \uc2e4\ud589 \ud750\ub984\ub300\ub85c \uc5b4\ub5bb\uac8c \ubcc0\ud654\ud558\ub294\uc9c0 \ud655\uc778\ud574\ubcfc \uac83\uc785\ub2c8\ub2e4."),(0,a.kt)("h2",{id:"\uc785\ub825-token-embedding--positional-encoding"},"\uc785\ub825 (Token Embedding + Positional Encoding)"),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\n# -----------------------------\n# Step 1: Token Embedding + Sinusoidal Positional Encoding\n# -----------------------------\nclass PositionalEncoding:\n    def __init__(self, max_len, d_model):\n        self.encoding = self.get_positional_encoding(max_len, d_model)\n\n    @staticmethod\n    def get_positional_encoding(max_len, d_model):\n        pos_enc = np.zeros((max_len, d_model))\n        for pos in range(max_len):\n            for i in range(0, d_model, 2):\n                pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n                if i + 1 < d_model:\n                    pos_enc[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / d_model)))\n        return pos_enc\n\n# \uc124\uc815\nvocab = {"the": 0, "cat": 1, "sat": 2, "on": 3, "mat": 4, ".": 5}\nsentence = ["the", "cat", "sat", "on", "the", "mat", "."]\nembedding_dim = 8  \nmax_len = len(sentence)\n\n# \uc784\ubca0\ub529\nembedding_layer = nn.Embedding(len(vocab), embedding_dim)\ntoken_indices = torch.tensor([vocab[word] for word in sentence])\ntoken_embeddings = embedding_layer(token_indices).detach().numpy()\n\n# Positional Encoding \ucd94\uac00\npos_encoding = PositionalEncoding(max_len, embedding_dim).encoding\ninput_embeddings = token_embeddings + pos_encoding\n\n# \ud788\ud2b8\ub9f5 \ucd9c\ub825 (Step 1)\nplt.figure(figsize=(10, 6))\nsns.heatmap(input_embeddings, annot=True, fmt=".2f", cmap="coolwarm", xticklabels=[f"Dim {i}" for i in range(embedding_dim)], yticklabels=sentence)\nplt.title("Step 1: Token Embeddings + Positional Encoding")\nplt.xlabel("Embedding Dimension")\nplt.ylabel("Tokens")\nplt.show()\n')),(0,a.kt)("h3",{id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0"},"\uacb0\uacfc \uc774\ubbf8\uc9c0"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"alt text",src:t(87458).Z,width:"1000",height:"600"})),(0,a.kt)("h2",{id:"layernorm-self-attention-\uc804\uc5d0-\uc815\uaddc\ud654"},"LayerNorm (Self-Attention \uc804\uc5d0 \uc815\uaddc\ud654)"),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\n# -----------------------------\n# Step 2: LayerNorm \uc801\uc6a9\n# -----------------------------\nlayer_norm = nn.LayerNorm(embedding_dim)\nnormalized_embeddings = layer_norm(torch.FloatTensor(input_embeddings)).detach().numpy()\n\n# \ud788\ud2b8\ub9f5 \ucd9c\ub825 (Step 2)\nplt.figure(figsize=(10, 6))\nsns.heatmap(normalized_embeddings, annot=True, fmt=".2f", cmap="coolwarm", xticklabels=[f"Dim {i}" for i in range(embedding_dim)], yticklabels=sentence)\nplt.title("Step 2: LayerNorm Applied (Before MHA)")\nplt.xlabel("Embedding Dimension")\nplt.ylabel("Tokens")\nplt.show()\n\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u2705 \ud655\uc778\ud560 \uc810",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"\ud3c9\uade0 0, \ubd84\uc0b0 1\ub85c \uc815\uaddc\ud654 \ub410\ub294\uac00"),(0,a.kt)("li",{parentName:"ul"},"\ud070 \uac12\uacfc \uc791\uc740 \uac12\uc774 \uc5b4\ub5bb\uac8c \ubcc0\ud654\ud588\ub098")))),(0,a.kt)("h3",{id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-1"},"\uacb0\uacfc \uc774\ubbf8\uc9c0"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"alt text",src:t(68771).Z,width:"1000",height:"600"})),(0,a.kt)("h2",{id:"multi-head-self-attention-\ubb38\uc7a5\uc5d0\uc11c-\uc911\uc694\ud55c-\ub2e8\uc5b4-\ucc3e\uae30"},"Multi-Head Self-Attention (\ubb38\uc7a5\uc5d0\uc11c \uc911\uc694\ud55c \ub2e8\uc5b4 \ucc3e\uae30)"),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# -----------------------------\n# Step 3: Multi-Head Self-Attention\n# -----------------------------\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0  # Head \uc218\uac00 \ub098\ub204\uc5b4 \ub5a8\uc5b4\uc9c0\ub3c4\ub85d \uc124\uc815\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        # Query, Key, Value \ud589\ub82c \uc0dd\uc131\n        self.W_query = nn.Linear(d_model, d_model)\n        self.W_key = nn.Linear(d_model, d_model)\n        self.W_value = nn.Linear(d_model, d_model)\n        self.out_proj = nn.Linear(d_model, d_model)  # \ucd5c\uc885 \ucd9c\ub825 \ubcc0\ud658\n\n    def forward(self, x):\n        batch_size, seq_length, d_model = x.shape\n        \n        # Query, Key, Value \uc0dd\uc131 \ubc0f Head \ubd84\ud560\n        Q = self.W_query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.W_key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.W_value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Scaled Dot-Product Attention \uc218\ud589\n        attn_scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        context = attn_weights @ V  # \uac12 \uc870\ud569\n\n        # Multi-Head Attention \uacb0\uacfc \uacb0\ud569\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, d_model)\n        return self.out_proj(context), attn_weights\n\n# \uc2e4\ud589\ntorch.manual_seed(42)\nnum_heads = 2  \nx = torch.FloatTensor(normalized_embeddings).unsqueeze(0)  # Batch \ucc28\uc6d0 \ucd94\uac00\n\n# Multi-Head Attention \uc2e4\ud589\nmha = MultiHeadAttention(embedding_dim, num_heads)\noutput, attention_weights = mha(x)\n\n# \uac00\ub85c\ub85c \uc5ec\ub7ec \uac1c\uc758 Attention Head\ub97c \ud55c \uc904\uc5d0 \uc2dc\uac01\ud654\nfig, axes = plt.subplots(1, num_heads, figsize=(6 * num_heads, 5))\n\nfor head in range(num_heads):\n    ax = axes[head] if num_heads > 1 else axes\n    sns.heatmap(attention_weights[0, head].detach().numpy(), annot=True, cmap="Blues",\n                xticklabels=sentence, yticklabels=sentence, ax=ax)\n    ax.set_title(f"Self-Attention Head {head + 1}")\n    ax.set_xlabel("Key Tokens")\n    ax.set_ylabel("Query Tokens")\n\nplt.tight_layout()\nplt.show()\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u2705 \ud655\uc778\ud560 \uc810",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Head1 \uacfc Head2\uac00 \uc5b4\ub5bb\uac8c \ud559\uc2b5 \ub410\ub294\uc9c0 \ud655\uc778\ud558\uae30")))),(0,a.kt)("h3",{id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-2"},"\uacb0\uacfc \uc774\ubbf8\uc9c0"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"alt text",src:t(70908).Z,width:"1200",height:"500"})),(0,a.kt)("h2",{id:"residual-connection-\uc6d0\ub798-\uc815\ubcf4-\uc720\uc9c0"},"Residual Connection (\uc6d0\ub798 \uc815\ubcf4 \uc720\uc9c0)"),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# -----------------------------\n# Step 4: Residual Connection\n# -----------------------------\n\n# Residual Connection \uc801\uc6a9\nresidual_output = x + output  # \uc6d0\ub798 \uc785\ub825(x) + MHA \ucd9c\ub825(output)\n\n# \ud788\ud2b8\ub9f5 \ucd9c\ub825 (Step 4)\nplt.figure(figsize=(10, 6))\nsns.heatmap(residual_output.squeeze(0).detach().numpy(), annot=True, fmt=".2f", cmap="coolwarm",\n            xticklabels=[f"Dim {i}" for i in range(embedding_dim)], yticklabels=sentence)\nplt.title("Step 4: Residual Connection Applied (After MHA)")\nplt.xlabel("Embedding Dimension")\nplt.ylabel("Tokens")\nplt.show()\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u2705 \ud655\uc778\ud560 \uc810",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"MHA\ub97c \ud1b5\uacfc \ud6c4 \ub108\ubb34 \uac12\uc774 \ucee4\uc9c0\uac70\ub098 \ubcc0\ud615\ub410\ub098?"),(0,a.kt)("li",{parentName:"ul"},"\ud2b9\uc815 \ucc28\uc6d0\uc5d0\uc11c \uac12\uc774 \uadf9\ub2e8\uc801\uc73c\ub85c \ubcc0\uacbd\ub410\ub098?")))),(0,a.kt)("h3",{id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-3"},"\uacb0\uacfc \uc774\ubbf8\uc9c0"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"alt text",src:t(12189).Z,width:"1000",height:"600"})),(0,a.kt)("h2",{id:"layernorm-ffn-\uc804\uc5d0-\uc815\uaddc\ud654"},"LayerNorm (FFN \uc804\uc5d0 \uc815\uaddc\ud654)"),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# -----------------------------\n# Step 5: Pre-LayerNorm \uc801\uc6a9 (Before FFN)\n# -----------------------------\n\n# 1\ufe0f\u20e3 FFN \uc804 LayerNorm \uc801\uc6a9 (Pre-LayerNorm \uc720\uc9c0)\nffn_layer_norm = nn.LayerNorm(embedding_dim)\nnormalized_ffn_input = ffn_layer_norm(residual_output)  # FFN \uc804 \uc815\uaddc\ud654\n\n# \ud788\ud2b8\ub9f5 \ucd9c\ub825 (Step 5)\nplt.figure(figsize=(10, 6))\nsns.heatmap(normalized_ffn_input.squeeze(0).detach().numpy(), annot=True, fmt=".2f", cmap="coolwarm",\n            xticklabels=[f"Dim {i}" for i in range(embedding_dim)], yticklabels=sentence)\nplt.title("Step 5: Pre-LayerNorm (Before FFN)")\nplt.xlabel("Embedding Dimension")\nplt.ylabel("Tokens")\nplt.show()\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u2705 \ud655\uc778\ud560 \uc810",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"FFN \uc804\uc5d0 LayerNorm\uc774 \uc5b4\ucf00 \uc801\uc6a9\ub410\ub098."),(0,a.kt)("li",{parentName:"ul"},"\ud3c9\uade0 0, \ubd84\uc0b0 1\uc778\uac00")))),(0,a.kt)("h3",{id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-4"},"\uacb0\uacfc \uc774\ubbf8\uc9c0"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"alt text",src:t(86183).Z,width:"1000",height:"600"})),(0,a.kt)("h2",{id:"feed-forward-network-ffn-self-attention-\uacb0\uacfc\ub97c-\uc815\uc81c"},"Feed Forward Network (FFN, Self-Attention \uacb0\uacfc\ub97c \uc815\uc81c)"),(0,a.kt)("hr",null),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# -----------------------------\n# Step 6: FFN \ub0b4\ubd80 \uacfc\uc815 (\ud655\uc7a5 \u2192 GELU \u2192 \ucd95\uc18c) \uc2dc\uac01\ud654\n# -----------------------------\n\n# 1\ufe0f\u20e3 FFN \ub808\uc774\uc5b4 \uc815\uc758\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, d_model, hidden_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, hidden_dim)  # \uc785\ub825 \ucc28\uc6d0 -> \ud655\uc7a5 \ucc28\uc6d0\n        self.gelu = nn.GELU()  # GELU \ud65c\uc131\ud654 \ud568\uc218 \uc801\uc6a9\n        self.fc2 = nn.Linear(hidden_dim, d_model)  # \ud655\uc7a5\ub41c \ucc28\uc6d0 -> \uc6d0\ub798 \ucc28\uc6d0\n\n    def forward(self, x):\n        x_expanded = self.fc1(x)  # \ud655\uc7a5\ub41c \ucc28\uc6d0 (32\ucc28\uc6d0)\n        x_gelu = self.gelu(x_expanded)  # GELU \ud65c\uc131\ud654 \uc801\uc6a9\n        x_output = self.fc2(x_gelu)  # \ub2e4\uc2dc \uc6d0\ub798 \ucc28\uc6d0\uc73c\ub85c \ucd95\uc18c\n        return x_expanded, x_gelu, x_output\n\n# 2\ufe0f\u20e3 FFN \uc2e4\ud589 (GELU \uc801\uc6a9)\nhidden_dim = 32  # \ub0b4\ubd80 \ud655\uc7a5 \ucc28\uc6d0 \uc124\uc815 (Transformer \uae30\ubcf8: 4\ubc30 \ud655\uc7a5)\nffn = FeedForwardNetwork(embedding_dim, hidden_dim)\n\n# FFN\uc758 \uac01 \ub2e8\uacc4\ubcc4 \ucd9c\ub825 \uc5bb\uae30\nexpanded_output, gelu_output, ffn_output = ffn(normalized_ffn_input)  # FFN \uc801\uc6a9\n\n# 3\ufe0f\u20e3 FFN \uacfc\uc815 \uac00\ub85c \ube44\uad50 \uadf8\ub798\ud504 \uc0dd\uc131\nfig, axes = plt.subplots(1, 4, figsize=(24, 6))\n\n# (1) FFN \uc785\ub825 (Pre-LayerNorm \ud6c4)\nsns.heatmap(normalized_ffn_input.squeeze(0).detch().numpy(),annot=True,fmt=".2f", cmap="coolwarm", ax=axes[0])\naxes[0].set_title("Input to FFN (Pre-LayerNorm)")\naxes[0].set_xlabel("Embedding Dimension")\naxes[0].set_ylabel("Tokens")\n\n# (2) \ud655\uc7a5\ub41c \ucc28\uc6d0 (fc1 \uc801\uc6a9 \ud6c4)\nsns.heatmap(expanded_output.squeeze(0).detach().numpy(), annot=False, cmap="coolwarm", ax=axes[1])\naxes[1].set_title("Expanded (fc1 Output, 32D)")\naxes[1].set_xlabel("Expanded Dimension")\naxes[1].set_ylabel("Tokens")\n\n# (3) GELU \ud65c\uc131\ud654 \uc801\uc6a9 \ud6c4\nsns.heatmap(gelu_output.squeeze(0).detach().numpy(), annot=False, cmap="coolwarm", ax=axes[2])\naxes[2].set_title("GELU Activation Applied")\naxes[2].set_xlabel("Expanded Dimension")\naxes[2].set_ylabel("Tokens")\n\n# (3) \ub2e4\uc2dc \uc6d0\ub798 \ucc28\uc6d0\uc73c\ub85c \ucd95\uc18c (fc2 \uc801\uc6a9 \ud6c4)\nsns.heatmap(ffn_output.squeeze(0).detach().numpy(), annot=True,fmt=".2f", cmap="coolwarm", ax=axes[3])\naxes[3].set_title("Reduced (fc2 Output, 8D)")\naxes[3].set_xlabel("Embedding Dimension")\naxes[3].set_ylabel("Tokens")\n\nplt.tight_layout()\nplt.show()\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u2705 \ud655\uc778\ud560 \uc810",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Layer \ud655\uc7a5"),(0,a.kt)("li",{parentName:"ul"},"GELU \ud65c\uc131\ud654 \ud6c4 \uac12\uc758 \ubcc0\ud654"),(0,a.kt)("li",{parentName:"ul"},"Layer \ucd95\uc18c")))),(0,a.kt)("h3",{id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-5"},"\uacb0\uacfc \uc774\ubbf8\uc9c0"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"alt text",src:t(45962).Z,width:"2400",height:"600"})),(0,a.kt)("h2",{id:"residual-connection-\uc6d0\ub798-\uc815\ubcf4-\uc720\uc9c0-1"},"Residual Connection (\uc6d0\ub798 \uc815\ubcf4 \uc720\uc9c0)"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# -----------------------------\n# Step 7: Residual Connection \uc801\uc6a9 (Final Output)\n# -----------------------------\n\n# 1\ufe0f\u20e3 Residual Connection \uc801\uc6a9 (FFN \ud6c4 \uc6d0\ub798 \uc815\ubcf4 \uc720\uc9c0)\nfinal_output = residual_output + ffn_output  # Residual \uc5f0\uacb0\n\n# 2\ufe0f\u20e3 Transformer \ube14\ub85d \ucd5c\uc885 \ucd9c\ub825 \ud655\uc778 (\ud788\ud2b8\ub9f5 \ucd9c\ub825)\nplt.figure(figsize=(10, 6))\nsns.heatmap(final_output.squeeze(0).detach().numpy(), annot=True, fmt=".2f", cmap="coolwarm",\n            xticklabels=[f"Dim {i}" for i in range(embedding_dim)], yticklabels=sentence)\nplt.title("Step 7: Final Output (After Residual Connection)")\nplt.xlabel("Embedding Dimension")\nplt.ylabel("Tokens")\nplt.show()\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u2705 \ud655\uc778\ud560 \uc810",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"\ucd5c\uc885 \ubca1\ud130\uac00 \uc798 \ub098\uc654\ub098")))),(0,a.kt)("h3",{id:"\uacb0\uacfc-\uc774\ubbf8\uc9c0-6"},"\uacb0\uacfc \uc774\ubbf8\uc9c0"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"alt text",src:t(31007).Z,width:"1000",height:"600"})))}p.isMDXComponent=!0},87458:(e,n,t)=>{t.d(n,{Z:()=>l});const l=t.p+"assets/images/tb1-6ec917a7b7265ed0cbf1a5a1400b2aea.png"},68771:(e,n,t)=>{t.d(n,{Z:()=>l});const l=t.p+"assets/images/tb2-f81a8054c8ae906a3ab144d6ebb21a8d.png"},70908:(e,n,t)=>{t.d(n,{Z:()=>l});const l=t.p+"assets/images/tb3-a0277d097e8e4572ac5d88d8a9d73559.png"},12189:(e,n,t)=>{t.d(n,{Z:()=>l});const l=t.p+"assets/images/tb4-f26b9f8b4da5580a67de4e4e54f009c4.png"},86183:(e,n,t)=>{t.d(n,{Z:()=>l});const l=t.p+"assets/images/tb5-638fd5cc5201e306e71dec36d2a0af9e.png"},45962:(e,n,t)=>{t.d(n,{Z:()=>l});const l=t.p+"assets/images/tb6-d9adaa02cffe1e7b01542f22db1e04ec.png"},31007:(e,n,t)=>{t.d(n,{Z:()=>l});const l=t.p+"assets/images/tb7-356c31ef501355e03920779438734acf.png"}}]);