"use strict";(self.webpackChunkblogsaurus=self.webpackChunkblogsaurus||[]).push([[26715],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>f});var o=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function l(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?l(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,o,r=function(e,n){if(null==e)return{};var t,o,r={},l=Object.keys(e);for(o=0;o<l.length;o++)t=l[o],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(o=0;o<l.length;o++)t=l[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var i=o.createContext({}),d=function(e){var n=o.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},m=function(e){var n=d(e.components);return o.createElement(i.Provider,{value:n},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},_=o.forwardRef((function(e,n){var t=e.components,r=e.mdxType,l=e.originalType,i=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=d(t),_=r,f=c["".concat(i,".").concat(_)]||c[_]||u[_]||l;return t?o.createElement(f,a(a({ref:n},m),{},{components:t})):o.createElement(f,a({ref:n},m))}));function f(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var l=t.length,a=new Array(l);a[0]=_;var s={};for(var i in n)hasOwnProperty.call(n,i)&&(s[i]=n[i]);s.originalType=e,s[c]="string"==typeof e?e:r,a[1]=s;for(var d=2;d<l;d++)a[d]=t[d];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}_.displayName="MDXCreateElement"},49569:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>a,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>d});var o=t(87462),r=(t(67294),t(3905));const l={sidebar_position:15},a="[LLM] 15. \ud1a0\ud070 \uc785\ub825 \ubd80\ud130 \ub2e8\uc5b4 \uc608\uce21\uae4c\uc9c0",s={unversionedId:"llm/llm15",id:"llm/llm15",title:"[LLM] 15. \ud1a0\ud070 \uc785\ub825 \ubd80\ud130 \ub2e8\uc5b4 \uc608\uce21\uae4c\uc9c0",description:"---",source:"@site/docs/llm/llm15.md",sourceDirName:"llm",slug:"/llm/llm15",permalink:"/docs/llm/llm15",draft:!1,tags:[],version:"current",sidebarPosition:15,frontMatter:{sidebar_position:15},sidebar:"aiSidebar",previous:{title:"[LLM] 14. Transformer Block \uc5f0\uacb0\ud558\uae30",permalink:"/docs/llm/llm14"}},i={},d=[{value:"\ud1a0\ud070 \uc784\ubca0\ub529 \ucf54\ub4dc",id:"\ud1a0\ud070-\uc784\ubca0\ub529-\ucf54\ub4dc",level:2},{value:"\uc2dc\uac01\ud654 \ud574\uc11c \ud655\uc778",id:"\uc2dc\uac01\ud654-\ud574\uc11c-\ud655\uc778",level:3},{value:"\uc804\uc5d0 \ub2e4\ub918\ub358 \ucf54\ub4dc",id:"\uc804\uc5d0-\ub2e4\ub918\ub358-\ucf54\ub4dc",level:2},{value:"Transformer Block",id:"transformer-block",level:3}],m={toc:d},c="wrapper";function u(e){let{components:n,...l}=e;return(0,r.kt)(c,(0,o.Z)({},m,l,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"llm-15-\ud1a0\ud070-\uc785\ub825-\ubd80\ud130-\ub2e8\uc5b4-\uc608\uce21\uae4c\uc9c0"},"[LLM]"," 15. \ud1a0\ud070 \uc785\ub825 \ubd80\ud130 \ub2e8\uc5b4 \uc608\uce21\uae4c\uc9c0"),(0,r.kt)("hr",null),(0,r.kt)("h2",{id:"\ud1a0\ud070-\uc784\ubca0\ub529-\ucf54\ub4dc"},"\ud1a0\ud070 \uc784\ubca0\ub529 \ucf54\ub4dc"),(0,r.kt)("hr",null),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"class GPTModel(nn.Module):\n    def __init__(self, vocab_size,d_model,num_heads, num_layers,dropout,context_length):\n        super().__init__()\n\n        # 1. Token embedding (\ub2e8\uc5b4 > \ubca1\ud130)\n        self.token_embedding = nn.Embedding(vocab_size,d_model)\n\n        # 2. position encoding\n        self.position_encoding = nn.Embedding(context_length, d_model)\n        \n        # 3. transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(d_model, num_heads, dropout, context_length) for _ in range(num_layers)\n        ])\n\n        # 4. Linear Projection\n        self.ln_final = nn.LayerNorm(d_model)\n        self.output_layer = nn.Linear(d_model,vocab_size)\n        \n    def forward(self, input_ids, mask):\n        # 1. token embedding \uc801\uc6a9\n        token_embeds = self.token_embedding(input_ids)\n\n        # 2. position embedding \uc801\uc6a9\n        pos_embeds = self.position_encoding(torch.arange(input_ids.shape[1],device=input_ids.device))\n\n        # token embedding + position encoding\n        x = token_embeds + pos_embeds\n\n        # 4. transformer block \ud1b5\uacfc\n        for block in self.blocks:\n            x = block(x, mask)\n        \n        # 5. \ucd5c\uc885\n        x = self.ln_final(x)\n        logits = self.output_layer(x)\n\n        return logits\n    \n")),(0,r.kt)("h3",{id:"\uc2dc\uac01\ud654-\ud574\uc11c-\ud655\uc778"},"\uc2dc\uac01\ud654 \ud574\uc11c \ud655\uc778"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# \ubaa8\ub378 \uc124\uc815\nvocab_size = 100    # \ub2e8\uc5b4 \uac1c\uc218\nd_model = 128       # \uc784\ubca0\ub529 \ucc28\uc6d0\ncontext_length = 20 # \ucd5c\ub300 \ubb38\uc7a5 \uae38\uc774\n\n# GPT \ubaa8\ub378 \uc0dd\uc131\nmodel = GPTModel(vocab_size,d_model,num_heads=4,num_layers=4,dropout=0.1,context_length=context_length)\n\n# \uac00\uc9dc \ub370\uc774\ud130\ninput_ids = torch.randint(0,vocab_size,(1,context_length))\n\nwith torch.no_grad():\n    token_embeds = model.token_embedding(input_ids).squeeze(0).numpy()  # (seq_length, d_model)\n    pos_embeds = model.position_encoding(torch.arange(context_length)).numpy()  # (seq_length, d_model)\n    combined_embeds = token_embeds + pos_embeds  # \ucd5c\uc885 Transformer \uc785\ub825\n\n# \ud83d\udd39 \ud788\ud2b8\ub9f5 \uc2dc\uac01\ud654 (Embedding \ud655\uc778)\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nsns.heatmap(token_embeds, cmap="coolwarm", ax=axes[0])\naxes[0].set_title("Token Embedding")\n\nsns.heatmap(pos_embeds, cmap="coolwarm", ax=axes[1])\naxes[1].set_title("Positional Encoding")\n\nsns.heatmap(combined_embeds, cmap="coolwarm", ax=axes[2])\naxes[2].set_title("Token + Positional Encoding")\n\nplt.show()\n')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"alt text",src:t(5912).Z,width:"1800",height:"500"})),(0,r.kt)("h2",{id:"\uc804\uc5d0-\ub2e4\ub918\ub358-\ucf54\ub4dc"},"\uc804\uc5d0 \ub2e4\ub918\ub358 \ucf54\ub4dc"),(0,r.kt)("hr",null),(0,r.kt)("h3",{id:"transformer-block"},"Transformer Block"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'class TransformerBlock(nn.Module):\n    """\n    d_model: \uc785\ub825 \ucc28\uc6d0 \ud06c\uae30\n    num_heads: Multi-Head Attention\uc5d0\uc11c \uc0ac\uc6a9\ud560 Attention Head \uac1c\uc218\n    dropout: \uacfc\uc801\ud569 \ubc29\uc9c0\ub97c \uc704\ud55c \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728\n    context_length: \ud55c\ubc88\uc5d0 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ubb38\uc7a5 \uae38\uc774\n    """\n    def __init__(self, d_model, num_heads, dropout, context_length):\n        super().__init__()\n\n        self.context_length = context_length\n        # Layer Normalization (Pre-LayerNorm \ubc29\uc2dd)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n        \n        # Multi-Head Attention (MHA)\n        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n\n        # Feed Forward Network (FFN)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),  # \ud655\uc7a5 (\uae30\ubcf8\uc801\uc73c\ub85c 4\ubc30 \ud06c\uae30\ub85c \uc99d\uac00)\n            nn.GELU(),  # \ud65c\uc131\ud654 \ud568\uc218\n            nn.Linear(d_model * 4, d_model),  # \ub2e4\uc2dc \uc6d0\ub798 \ud06c\uae30\ub85c \ucd95\uc18c\n            nn.Dropout(dropout),\n        )\n\n        # \ubbf8\ub9ac causal mask\ub97c \ub9cc\ub4e4\uc5b4\ub450\uae30\n        self.register_buffer("causal_mask", self._create_causal_mask(context_length))\n\n    def _create_causal_mask(self, seq_length):\n        """\n        \uc0c1\uc0bc\uac01 \ud589\ub82c\uc744 \ub9cc\ub4e4\uc5b4\uc11c \ubbf8\ub798 \ub2e8\uc5b4\ub97c \ubcf4\uc9c0 \ubabb\ud558\ub3c4\ub85d \ud568.\n        """\n        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float(\'-inf\'))\n        return mask\n\n    def forward(self, x, mask=None):\n        # \uc785\ub825 \uc2dc\ud000\uc2a4 \ud655\uc778\n        seq_length = x.size(1)\n        if mask is None:\n            mask = self._create_causal_mask(seq_length).to(x.device) \n\n        # Multi-Head Attention (Self-Attention)\n        x = self.ln1(x)\n        attn_out,_ = self.attn(x,x,x,attn_mask=mask,need_weights=False)\n        x = x + attn_out # residual_connection\n\n        # Feed Forward Network (FFN)\n        x = self.ln2(x)\n        ffn_out = self.ffn(x)\n        x = x + ffn_out # residual_connection\n        \n        return x\n')))}u.isMDXComponent=!0},5912:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/1-6fff041c825efa18ff6a1603b3fc6d44.png"}}]);