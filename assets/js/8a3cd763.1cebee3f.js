"use strict";(self.webpackChunkblogsaurus=self.webpackChunkblogsaurus||[]).push([[77766],{3905:(n,e,t)=>{t.d(e,{Zo:()=>m,kt:()=>p});var r=t(67294);function o(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function a(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(n);e&&(r=r.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,r)}return t}function l(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?a(Object(t),!0).forEach((function(e){o(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}function s(n,e){if(null==n)return{};var t,r,o=function(n,e){if(null==n)return{};var t,r,o={},a=Object.keys(n);for(r=0;r<a.length;r++)t=a[r],e.indexOf(t)>=0||(o[t]=n[t]);return o}(n,e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(n);for(r=0;r<a.length;r++)t=a[r],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(n,t)&&(o[t]=n[t])}return o}var i=r.createContext({}),u=function(n){var e=r.useContext(i),t=e;return n&&(t="function"==typeof n?n(e):l(l({},e),n)),t},m=function(n){var e=u(n.components);return r.createElement(i.Provider,{value:e},n.children)},d="mdxType",c={inlineCode:"code",wrapper:function(n){var e=n.children;return r.createElement(r.Fragment,{},e)}},f=r.forwardRef((function(n,e){var t=n.components,o=n.mdxType,a=n.originalType,i=n.parentName,m=s(n,["components","mdxType","originalType","parentName"]),d=u(t),f=o,p=d["".concat(i,".").concat(f)]||d[f]||c[f]||a;return t?r.createElement(p,l(l({ref:e},m),{},{components:t})):r.createElement(p,l({ref:e},m))}));function p(n,e){var t=arguments,o=e&&e.mdxType;if("string"==typeof n||o){var a=t.length,l=new Array(a);l[0]=f;var s={};for(var i in e)hasOwnProperty.call(e,i)&&(s[i]=e[i]);s.originalType=n,s[d]="string"==typeof n?n:o,l[1]=s;for(var u=2;u<a;u++)l[u]=t[u];return r.createElement.apply(null,l)}return r.createElement.apply(null,t)}f.displayName="MDXCreateElement"},33210:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>i,contentTitle:()=>l,default:()=>c,frontMatter:()=>a,metadata:()=>s,toc:()=>u});var r=t(87462),o=(t(67294),t(3905));const a={sidebar_position:14},l="[LLM] 14. Transformer Block \uc5f0\uacb0\ud558\uae30",s={unversionedId:"llm/llm14",id:"llm/llm14",title:"[LLM] 14. Transformer Block \uc5f0\uacb0\ud558\uae30",description:"---",source:"@site/docs/llm/llm14.md",sourceDirName:"llm",slug:"/llm/llm14",permalink:"/docs/llm/llm14",draft:!1,tags:[],version:"current",sidebarPosition:14,frontMatter:{sidebar_position:14},sidebar:"aiSidebar",previous:{title:"[LLM] 13. Transformer Block \uc2e4\uc2b5",permalink:"/docs/llm/llm13"},next:{title:"[LLM] 15. \ud1a0\ud070 \uc785\ub825 \ubd80\ud130 \ub2e8\uc5b4 \uc608\uce21\uae4c\uc9c0",permalink:"/docs/llm/llm15"}},i={},u=[{value:"Transformer Block \uc2e4\uc2b5 \ucf54\ub4dc",id:"transformer-block-\uc2e4\uc2b5-\ucf54\ub4dc",level:2},{value:"\ud750\ub984",id:"\ud750\ub984",level:3},{value:"\uc2dc\uac01\ud654",id:"\uc2dc\uac01\ud654",level:3},{value:"Casual Mask \uc801\uc6a9",id:"casual-mask-\uc801\uc6a9",level:3}],m={toc:u},d="wrapper";function c(n){let{components:e,...a}=n;return(0,o.kt)(d,(0,r.Z)({},m,a,{components:e,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"llm-14-transformer-block-\uc5f0\uacb0\ud558\uae30"},"[LLM]"," 14. Transformer Block \uc5f0\uacb0\ud558\uae30"),(0,o.kt)("hr",null),(0,o.kt)("p",null,"Transformer Block\uc744 \uc5f0\uacb0\ud558\uc5ec GPT \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4 \ubcf4\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4."),(0,o.kt)("h2",{id:"transformer-block-\uc2e4\uc2b5-\ucf54\ub4dc"},"Transformer Block \uc2e4\uc2b5 \ucf54\ub4dc"),(0,o.kt)("hr",null),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import torch\nimport torch.nn as nn\n\nclass TransformerBlock(nn.Module):\n    """\n    d_model: \uc785\ub825 \ucc28\uc6d0 \ud06c\uae30\n    num_heads: Multi-Head Attention\uc5d0\uc11c \uc0ac\uc6a9\ud560 Attention Head \uac1c\uc218\n    dropout: \uacfc\uc801\ud569 \ubc29\uc9c0\ub97c \uc704\ud55c \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728\n    context_length: \ud55c\ubc88\uc5d0 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ubb38\uc7a5 \uae38\uc774\n    """\n    def __init__(self, d_model, num_heads, dropout, context_length):\n        super().__init__()\n        \n        # Layer Normalization (Pre-LayerNorm \ubc29\uc2dd)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n        \n        # Multi-Head Attention (MHA)\n        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n\n        # Feed Forward Network (FFN)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),  # \ud655\uc7a5 (\uae30\ubcf8\uc801\uc73c\ub85c 4\ubc30 \ud06c\uae30\ub85c \uc99d\uac00)\n            nn.GELU(),  # \ud65c\uc131\ud654 \ud568\uc218\n            nn.Linear(d_model * 4, d_model),  # \ub2e4\uc2dc \uc6d0\ub798 \ud06c\uae30\ub85c \ucd95\uc18c\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x, mask=None):\n        # Multi-Head Attention (Self-Attention)\n        x = self.ln1(x)\n        attn_out,_ = self.attn(x,x,x,attn_mask=mask,need_weights=False)\n        x = x + attn_out # residual_connection\n\n        # Feed Forward Network (FFN)\n        x = self.ln2(x)\n        ffn_out = self.ffn(x)\n        x = x + ffn_out # residual_connection\n        \n        return x\n\n')),(0,o.kt)("h3",{id:"\ud750\ub984"},"\ud750\ub984"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-text"},"Pre-LayerNorm(For Self-Attention) > Self-Attention > Residual connection >Pre-LayerNorm(For FFN) >  FFN > Residual connection \n")),(0,o.kt)("h3",{id:"\uc2dc\uac01\ud654"},"\uc2dc\uac01\ud654"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\n# \ubaa8\ub378 \uc124\uc815\nd_model = 128   # \uc784\ubca0\ub529 \ucc28\uc6d0\nnum_heads = 4   # Attention Head \uac1c\uc218\nnum_layers = 4  # Transformer Block \uac1c\uc218\ndropout = 0.1   # \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728\ncontext_length = 20  # \ubb38\uc7a5\uc758 \ucd5c\ub300 \uae38\uc774\n\n# \uc5ec\ub7ec \uac1c\uc758 Transformer Block \uc313\uae30\ntransformer_blocks = nn.ModuleList([\n    TransformerBlock(d_model,num_heads,dropout,context_length) for _ in range(num_layers)\n])\n\n# \uac00\uc9dc \uc785\ub825 \ub370\uc774\ud130 \uc0dd\uc131\nx = torch.randn(1,context_length,d_model)\nprint("\uc785\ub825 \ub370\uc774\ud130 \ud06c\uae30:",x.shape)\n\n\n# Transformer Block\uc744 \ud1b5\uacfc\ud558\uba74\uc11c \ubca1\ud130 \ubcc0\ud654 \uc800\uc7a5\noutputs = [x.squeeze(0).detach().numpy()]  # \ucd08\uae30 \uc785\ub825 \uc800\uc7a5\n\nfor i, block in enumerate(transformer_blocks):\n    x = block(x)\n    print(f"Block {i+1} \ud1b5\uacfc \ud6c4 \ucd9c\ub825 \ud06c\uae30:",x.shape)\n    outputs.append(x.squeeze(0).detach().numpy())\n\n# \ud788\ud2b8\ub9f5\n# \ud788\ud2b8\ub9f5 \uc2dc\uac01\ud654\nfig, axes = plt.subplots(1, num_layers + 1, figsize=(20, 5))\n\nfor i, output in enumerate(outputs):\n    sns.heatmap(output, cmap="coolwarm", ax=axes[i])\n    axes[i].set_title(f"Step {i} (After {i} Blocks)")\n\nplt.show()\n')),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"alt text",src:t(68265).Z,width:"2000",height:"500"})),(0,o.kt)("h3",{id:"casual-mask-\uc801\uc6a9"},"Casual Mask \uc801\uc6a9"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'class TransformerBlock(nn.Module):\n    """\n    d_model: \uc785\ub825 \ucc28\uc6d0 \ud06c\uae30\n    num_heads: Multi-Head Attention\uc5d0\uc11c \uc0ac\uc6a9\ud560 Attention Head \uac1c\uc218\n    dropout: \uacfc\uc801\ud569 \ubc29\uc9c0\ub97c \uc704\ud55c \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728\n    context_length: \ud55c\ubc88\uc5d0 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ubb38\uc7a5 \uae38\uc774\n    """\n    def __init__(self, d_model, num_heads, dropout, context_length):\n        super().__init__()\n\n        self.context_length = context_length\n        # Layer Normalization (Pre-LayerNorm \ubc29\uc2dd)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n        \n        # Multi-Head Attention (MHA)\n        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout, batch_first=True)\n\n        # Feed Forward Network (FFN)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),  # \ud655\uc7a5 (\uae30\ubcf8\uc801\uc73c\ub85c 4\ubc30 \ud06c\uae30\ub85c \uc99d\uac00)\n            nn.GELU(),  # \ud65c\uc131\ud654 \ud568\uc218\n            nn.Linear(d_model * 4, d_model),  # \ub2e4\uc2dc \uc6d0\ub798 \ud06c\uae30\ub85c \ucd95\uc18c\n            nn.Dropout(dropout),\n        )\n\n        # \ubbf8\ub9ac causal mask\ub97c \ub9cc\ub4e4\uc5b4\ub450\uae30\n        self.register_buffer("causal_mask", self._create_causal_mask(context_length))\n\n    def _create_causal_mask(self, seq_length):\n        """\n        \uc0c1\uc0bc\uac01 \ud589\ub82c\uc744 \ub9cc\ub4e4\uc5b4\uc11c \ubbf8\ub798 \ub2e8\uc5b4\ub97c \ubcf4\uc9c0 \ubabb\ud558\ub3c4\ub85d \ud568.\n        """\n        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float(\'-inf\'))\n        return mask\n\n    def forward(self, x, mask=None):\n        # \uc785\ub825 \uc2dc\ud000\uc2a4 \ud655\uc778\n        seq_length = x.size(1)\n        if mask is None:\n            mask = self._create_causal_mask(seq_length).to(x.device) \n\n        # Multi-Head Attention (Self-Attention)\n        x = self.ln1(x)\n        attn_out,_ = self.attn(x,x,x,attn_mask=mask,need_weights=False)\n        x = x + attn_out # residual_connection\n\n        # Feed Forward Network (FFN)\n        x = self.ln2(x)\n        ffn_out = self.ffn(x)\n        x = x + ffn_out # residual_connection\n        \n        return x\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\n# \ubaa8\ub378 \uc124\uc815\nd_model = 128   # \uc784\ubca0\ub529 \ucc28\uc6d0\nnum_heads = 4   # Attention Head \uac1c\uc218\nnum_layers = 4  # Transformer Block \uac1c\uc218\ndropout = 0.1   # \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728\ncontext_length = 20  # \ubb38\uc7a5\uc758 \ucd5c\ub300 \uae38\uc774\n\n# \uc5ec\ub7ec \uac1c\uc758 Transformer Block \uc313\uae30\ntransformer_blocks = nn.ModuleList([\n    TransformerBlock(d_model,num_heads,dropout,context_length) for _ in range(num_layers)\n])\n\n# \uac00\uc9dc \uc785\ub825 \ub370\uc774\ud130 \uc0dd\uc131\nx = torch.randn(1,context_length,d_model)\nprint("\uc785\ub825 \ub370\uc774\ud130 \ud06c\uae30:",x.shape)\n\n# Transformer Block\uc744 \ud1b5\uacfc\ud558\uba74\uc11c \ubca1\ud130 \ubcc0\ud654 \uc800\uc7a5\noutputs = [x.squeeze(0).detach().numpy()]  # \ucd08\uae30 \uc785\ub825 \uc800\uc7a5\n\nfor i, block in enumerate(transformer_blocks):\n    x = block(x)  # Causal Mask \uc801\uc6a9\n    outputs.append(x.squeeze(0).detach().numpy())  # \uacb0\uacfc \uc800\uc7a5\n    print(f"Block {i+1} \ud1b5\uacfc \ud6c4 \ucd9c\ub825 \ud06c\uae30:", x.shape)  # (1, 20, 128) \uc720\uc9c0 \ud655\uc778\n\n# \ud788\ud2b8\ub9f5\n# \ud788\ud2b8\ub9f5 \uc2dc\uac01\ud654\nfig, axes = plt.subplots(1, num_layers + 1, figsize=(20, 5))\n\nfor i, output in enumerate(outputs):\n    sns.heatmap(output, cmap="coolwarm", ax=axes[i])\n    axes[i].set_title(f"Step {i} (After {i} Blocks)")\n\nplt.show()\n\n')),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"alt text",src:t(53308).Z,width:"2000",height:"500"})))}c.isMDXComponent=!0},68265:(n,e,t)=>{t.d(e,{Z:()=>r});const r=t.p+"assets/images/1-386b3414211f2ff63827f90a55a5b6e4.png"},53308:(n,e,t)=>{t.d(e,{Z:()=>r});const r=t.p+"assets/images/2-8afa6ca0ac1941c254256389154b1f57.png"}}]);