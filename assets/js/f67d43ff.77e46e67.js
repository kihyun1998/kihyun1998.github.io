"use strict";(self.webpackChunkblogsaurus=self.webpackChunkblogsaurus||[]).push([[8944],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>f});var a=n(67294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var s=a.createContext({}),d=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=d(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=d(n),c=l,f=p["".concat(s,".").concat(c)]||p[c]||m[c]||r;return n?a.createElement(f,i(i({ref:t},u),{},{components:n})):a.createElement(f,i({ref:t},u))}));function f(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=n.length,i=new Array(r);i[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[p]="string"==typeof e?e:l,i[1]=o;for(var d=2;d<r;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},58862:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var a=n(87462),l=(n(67294),n(3905));const r={sidebar_position:11},i="[LLM] 11. Multi-Head Attention",o={unversionedId:"llm/llm11",id:"llm/llm11",title:"[LLM] 11. Multi-Head Attention",description:"---",source:"@site/docs/llm/llm11.md",sourceDirName:"llm",slug:"/llm/llm11",permalink:"/docs/llm/llm11",draft:!1,tags:[],version:"current",sidebarPosition:11,frontMatter:{sidebar_position:11},sidebar:"aiSidebar",previous:{title:"[LLM] 10. Casual Attention",permalink:"/docs/llm/llm10"}},s={},d=[{value:"Multi-Head Attention",id:"multi-head-attention",level:2},{value:"\uac04\ub2e8\ud55c \uc608\uc2dc",id:"\uac04\ub2e8\ud55c-\uc608\uc2dc",level:3},{value:"\uc65c \uc4f0\ub098?",id:"\uc65c-\uc4f0\ub098",level:3},{value:"\uadf8\ub798\uc11c \ub2e4\uc591\ud55c \uad00\uc810\uc744 \ub204\uac00 \uc815\ud558\ub294\ub370",id:"\uadf8\ub798\uc11c-\ub2e4\uc591\ud55c-\uad00\uc810\uc744-\ub204\uac00-\uc815\ud558\ub294\ub370",level:3},{value:"\uc2e4\uc2b5 \ucf54\ub4dc",id:"\uc2e4\uc2b5-\ucf54\ub4dc",level:2},{value:"\uacb0\ub860",id:"\uacb0\ub860",level:3}],u={toc:d},p="wrapper";function m(e){let{components:t,...r}=e;return(0,l.kt)(p,(0,a.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"llm-11-multi-head-attention"},"[LLM]"," 11. Multi-Head Attention"),(0,l.kt)("hr",null),(0,l.kt)("h2",{id:"multi-head-attention"},"Multi-Head Attention"),(0,l.kt)("hr",null),(0,l.kt)("p",null,"Multi Head Attention\uc774 \ubb50\ub0d0\uba74 \uc5b4\ub824\uc6b4 \ub9d0\ub85c \ud558\uc790\uba74 Self-Attention\uc744 \uc5ec\ub7ec \uac1c \ubcd1\ub82c\ub85c \uc218\ud589\ud558\uc5ec \ub354 \ud48d\ubd80\ud55c \ubb38\ub9e5 \uc815\ubcf4\ub97c \ud559\uc2b5\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uac83\uc785\ub2c8\ub2e4."),(0,l.kt)("p",null,"\uac04\ub2e8\ud558\uac8c \ub9d0\ud558\uba74 Self-Attention\uc744 \ud558\uba74 \ud558\ub098\uc758 \uad00\uc810\uc5d0\uc11c\ub9cc Score\uac00 \ub098\uc624\ub294\ub370 \uadf8\ub7ec\uba74 \uc5ec\ub7ec \uad00\uc810\uc744 \uac16\uc9c0 \ubabb\ud558\ub2c8\uae4c \uc5ec\ub7ec \uad00\uc810\uc5d0\uc11c \uc810\uc218\ub97c \uad6c\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uac83\uc774 Multi-Head-Attention \uc785\ub2c8\ub2e4."),(0,l.kt)("h3",{id:"\uac04\ub2e8\ud55c-\uc608\uc2dc"},"\uac04\ub2e8\ud55c \uc608\uc2dc"),(0,l.kt)("p",null,"\uc544\ub798 \ubb38\uc7a5\uc774 \uc788\uc744 \ub54c Multi-Head-Attnetion\uc744 \ud55c\ub2e4\uace0 \ubcf4\uba74"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},'"\uac15\uc544\uc9c0\uac00 \uacf5\uc6d0\uc5d0\uc11c \uc2e0\ub098\uac8c \ub6f0\uc5b4\ub2e4\ub2cc\ub2e4"',(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},'  - "\uac15\uc544\uc9c0"\uac00 \ubb50\ud558\ub2c8? -> "\ub6f0\uc5b4\ub2e4\ub2cc\ub2e4"\n  - "\uc5b4\ub514\uc5d0\uc11c?" -> "\uacf5\uc6d0\uc5d0\uc11c"\n  - "\uae30\ubd84\uc774 \uc5b4\ub54c?" -> "\uc2e0\ub098\uac8c"\n')))),(0,l.kt)("p",null,"\uc640 \uac19\uc2b5\ub2c8\ub2e4."),(0,l.kt)("h3",{id:"\uc65c-\uc4f0\ub098"},"\uc65c \uc4f0\ub098?"),(0,l.kt)("p",null,"Self-Attention\ub9cc \uc0ac\uc6a9\ud558\uba74 \ud1a0\ud070\uc744 \ud55c\uac00\uc9c0 \ubc29\uc2dd\uc73c\ub85c\ub9cc \uc774\ud574\ud569\ub2c8\ub2e4. Multi-Head Attention\uc744 \uc0ac\uc6a9\ud558\uba74 \uc5ec\ub7ec \uac1c\uc758 Self-Attention\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uacfc \uac19\uae30\uc5d0 \ub2e4\uc591\ud55c \ud328\ud134\uc744 \ud559\uc2b5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4."),(0,l.kt)("h3",{id:"\uadf8\ub798\uc11c-\ub2e4\uc591\ud55c-\uad00\uc810\uc744-\ub204\uac00-\uc815\ud558\ub294\ub370"},"\uadf8\ub798\uc11c \ub2e4\uc591\ud55c \uad00\uc810\uc744 \ub204\uac00 \uc815\ud558\ub294\ub370"),(0,l.kt)("p",null,'Q,K,V \ud589\ub82c\uc774 \uc815\ud55c\ub2e4\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Self-Attention\ub3c4 \uc758\ubbf8 \ud30c\uc545\uc744 \uc704\ud574 Q,K,V\uac00 \uad00\uc5ec\ud588\uc796\uc544\uc694. \uadf8\uac70\uc640 \uac19\uc544\uc694.\n\uc774 \ubb38\uc7a5\ub9cc \ubcf4\uba74 \ud55c\uac00\uc9c0 \uc758\ubb38\uc774 \ub354 \ub4e4\uc218 \uc788\uc2b5\ub2c8\ub2e4. "\uadf8\ub7fc \uc5b4\ub5a4 \uad00\uc810\uc778\uc9c0\ub97c \uc5b4\ub5bb\uac8c \uc815\ud558\ub098?"\ub77c\ub294 \uc758\ubb38\uc774 \ub4e4\uc5c8\uc5c8\ub294\ub370. \ucc3e\uc544\ubcf4\ub2c8 \uc774\ub294 \uc0ac\ub78c\uc774 \uc9c0\uc815\ud574\uc904 \uc218\ub294 \uc5c6\uace0 \ub79c\ub364\ud55c \uac12\uc73c\ub85c \uc2dc\uc791\ud558\uc5ec. \uc5ed\uc804\ud30c\ub97c \ud1b5\ud55c \ubbf8\uc138\uc870\uc815\uc774 \uc774\ub8e8\uc5b4\uc838\uc57c \ud55c\ub2e4\uace0 \ud569\ub2c8\ub2e4.'),(0,l.kt)("h2",{id:"\uc2e4\uc2b5-\ucf54\ub4dc"},"\uc2e4\uc2b5 \ucf54\ub4dc"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import ace_tools as tools\n\n# Multi-Head Attention \uad6c\ud604\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0  # Head \uc218\uac00 \ub098\ub204\uc5b4 \ub5a8\uc5b4\uc9c0\ub3c4\ub85d \uc124\uc815\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        # Query, Key, Value \ud589\ub82c \uc0dd\uc131\n        self.W_query = nn.Linear(d_model, d_model)\n        self.W_key = nn.Linear(d_model, d_model)\n        self.W_value = nn.Linear(d_model, d_model)\n        self.out_proj = nn.Linear(d_model, d_model)  # \ucd5c\uc885 \ucd9c\ub825 \ubcc0\ud658\n\n    def forward(self, x):\n        batch_size, seq_length, d_model = x.shape\n        \n        # Query, Key, Value \uc0dd\uc131 \ubc0f Head \ubd84\ud560\n        Q = self.W_query(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.W_key(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.W_value(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Scaled Dot-Product Attention \uc218\ud589\n        attn_scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        context = attn_weights @ V  # \uac12 \uc870\ud569\n\n        # Multi-Head Attention \uacb0\uacfc \uacb0\ud569\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, d_model)\n        return self.out_proj(context), attn_weights\n\n# \ub79c\ub364 \uc785\ub825 \ub370\uc774\ud130 \uc0dd\uc131 \ubc0f \uc2e4\ud589 \uc608\uc81c\ntorch.manual_seed(42)\nbatch_size, seq_length, d_model, num_heads = 1, 6, 8, 2  # \ud55c \ubb38\uc7a5\uc5d0 6\uac1c\uc758 \ud1a0\ud070, 8\ucc28\uc6d0 \ubca1\ud130, 2\uac1c\uc758 Head \uc0ac\uc6a9\nx = torch.randn(batch_size, seq_length, d_model)  # \ub79c\ub364 \uc785\ub825 \ub370\uc774\ud130\n\n# Multi-Head Attention \uc2e4\ud589\nmha = MultiHeadAttention(d_model, num_heads)\noutput, attention_weights = mha(x)\n\n# Attention \uac00\uc911\uce58 \uc2dc\uac01\ud654\nfor head in range(num_heads):\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(attention_weights[0, head].detach().numpy(), annot=True, cmap="Blues", xticklabels=[f"T{i}" for i in range(seq_length)], yticklabels=[f"T{i}" for i in range(seq_length)])\n    plt.title(f"Attention Head {head + 1}")\n    plt.xlabel("Key Tokens")\n    plt.ylabel("Query Tokens")\n    plt.show()\n\n# Attention \uac00\uc911\uce58 \ub370\uc774\ud130\ud504\ub808\uc784 \ucd9c\ub825\nattn_df_head1 = pd.DataFrame(attention_weights[0, 0].detach().numpy(), columns=[f"T{i}" for i in range(seq_length)])\nattn_df_head2 = pd.DataFrame(attention_weights[0, 1].detach().numpy(), columns=[f"T{i}" for i in range(seq_length)])\n\n# tools.display_dataframe_to_user(name="Attention Weights - Head 1", dataframe=attn_df_head1)\n# tools.display_dataframe_to_user(name="Attention Weights - Head 2", dataframe=attn_df_head2)\n\nprint("Attention Weights - Head 1")\nprint(attn_df_head1)\nprint("\\nAttention Weights - Head 2")\nprint(attn_df_head2)\n')),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"alt text",src:n(39521).Z,width:"600",height:"500"})),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"alt text",src:n(7274).Z,width:"600",height:"500"})),(0,l.kt)("p",null,"\ubcf4\uba74 Head1\uacfc Head2\uc758 \uc5f0\uad00\ub3c4 \ud328\ud134\uc774 \ub2e4\ub978\uac78 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4."),(0,l.kt)("h3",{id:"\uacb0\ub860"},"\uacb0\ub860"),(0,l.kt)("p",null,"Multi-Head Attention\uc740 Self-Attention\ub9cc \uc4f0\uba74 \ud6a8\uc728\uc774 \uc548\ub098\uc624\ub2c8 \ubcd1\ub82c\ucc98\ub9ac \ud574\uac00\uc9c0\uad6c \ud6a8\uc728\uc744 \uadf9\ub300\ud654\ud55c \ubc29\ubc95 \uac19\uc2b5\ub2c8\ub2e4."))}m.isMDXComponent=!0},39521:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Figure_mha-a08636917ec077c716c73adf0fa6d768.png"},7274:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Figure_mha2-fefbe9796ae55d82a6e2ff582687027d.png"}}]);